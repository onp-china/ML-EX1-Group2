# è®­ç»ƒä»£ç è¯´æ˜

> **å„é˜¶æ®µæ¨¡å‹è®­ç»ƒè„šæœ¬è¯¦è§£**

æœ¬ç›®å½•åŒ…å«äº†ä»åŸºçº¿åˆ°æœ€ç»ˆStackingé›†æˆçš„æ‰€æœ‰è®­ç»ƒä»£ç ï¼Œå±•ç¤ºäº†æ¨¡å‹æ¶æ„çš„æ¼”è¿›å’Œè®­ç»ƒç­–ç•¥çš„ä¼˜åŒ–ã€‚

---

## ğŸ“ è®­ç»ƒè„šæœ¬æ¦‚è§ˆ

### Stage 1: ç‰¹å¾èåˆé©å‘½ (57%â†’85%)

**æš‚æ— ç‹¬ç«‹è®­ç»ƒè„šæœ¬** - ImprovedV2æ¨¡å‹é€šè¿‡å®éªŒè¿­ä»£è·å¾—

**å…³é”®åˆ›æ–°**:
- 6ç§ç‰¹å¾èåˆæ–¹å¼
- CBAMæ³¨æ„åŠ›æœºåˆ¶
- è‡ªé€‚åº”åŠ æƒèåˆ

---

### Stage 2: æ·±åº¦ä¼˜åŒ–çªç ´ (85%â†’89%)

#### `train_88_83_multi_seed.py` â­ **æ ¸å¿ƒè®­ç»ƒè„šæœ¬**

**ç”¨é€”**: è®­ç»ƒResNetç³»åˆ—æ¨¡å‹ï¼Œç›®æ ‡88.83%å‡†ç¡®ç‡

**å…³é”®ç‰¹æ€§**:
- ResNetCompareNetæ¶æ„
- Focal LossæŸå¤±å‡½æ•°
- AdamWä¼˜åŒ–å™¨
- æ··åˆç²¾åº¦è®­ç»ƒ (AMP)
- æ—©åœæœºåˆ¶

**æ¨¡å‹é…ç½®**:
```python
# æ¶æ„å‚æ•°
layers = [2, 2, 2]  # æˆ– [3, 3, 3]
feat_dim = 256
width_mult = 1.0

# è®­ç»ƒå‚æ•°
optimizer = AdamW(lr=1e-3, weight_decay=5e-4)
criterion = FocalLoss(alpha=1.0, gamma=2.0)
scheduler = ReduceLROnPlateau(patience=3)
```

**ä½¿ç”¨æ–¹æ³•**:
```bash
# è®­ç»ƒResNet-Optimized-1.12 (æœ€ä½³å•æ¨¡å‹)
python scripts/training/train_88_83_multi_seed.py --layers 3,3,3 --epochs 100

# è®­ç»ƒResNet-Fusion
python scripts/training/train_88_83_multi_seed.py --layers 2,2,2 --use_fusion

# è®­ç»ƒResNet-Optimized
python scripts/training/train_88_83_multi_seed.py --layers 2,2,2
```

---

### Stage 3: å¤šæ ·æ€§æ¢ç´¢ (84%â†’88%)

#### `train_multi_seed_optimized.py` â­ **å¤šç§å­è®­ç»ƒ**

**ç”¨é€”**: ä½¿ç”¨ä¸åŒéšæœºç§å­è®­ç»ƒæ¨¡å‹ï¼Œå¢åŠ å¤šæ ·æ€§

**å…³é”®ç‰¹æ€§**:
- å¤šç§å­è®­ç»ƒ (42, 2023, 2024, 2025)
- ä¸åŒç½‘ç»œå®½åº¦æ¢ç´¢
- å¹¶è¡Œè®­ç»ƒæ”¯æŒ

**ä½¿ç”¨æ–¹æ³•**:
```bash
# è®­ç»ƒæ‰€æœ‰ç§å­
python scripts/training/train_multi_seed_optimized.py --seeds 42,2023,2024,2025

# è®­ç»ƒå•ä¸ªç§å­
python scripts/training/train_multi_seed_optimized.py --seeds 2025
```

#### `train_simple_multi_seed.py` **ç®€åŒ–å¤šç§å­è®­ç»ƒ**

**ç”¨é€”**: è½»é‡çº§å¤šç§å­è®­ç»ƒè„šæœ¬

**ç‰¹ç‚¹**:
- æ›´ç®€å•çš„é…ç½®
- å¿«é€Ÿå®éªŒ
- é€‚åˆè°ƒè¯•

#### `train_efficientnet_symmetric.py` **EfficientNetè®­ç»ƒ**

**ç”¨é€”**: è®­ç»ƒEfficientNetæ¶æ„æ¨¡å‹

**å…³é”®ç‰¹æ€§**:
- EfficientNet-B0æ¶æ„
- å¯¹ç§°æ•°æ®å¢å¼º
- æ ‡ç­¾å¹³æ»‘

**ä½¿ç”¨æ–¹æ³•**:
```bash
python scripts/training/train_efficientnet_symmetric.py --epochs 50
```

---

### Stage 4: Stackingé›†æˆ (89%â†’93%)

#### `run_stacking.py` â­ **Stackingé›†æˆè®­ç»ƒ**

**ç”¨é€”**: è®­ç»ƒLightGBMå…ƒå­¦ä¹ å™¨ï¼Œé›†æˆ10ä¸ªåŸºç¡€æ¨¡å‹

**å…³é”®ç‰¹æ€§**:
- 10ä¸ªå¼‚æ„åŸºç¡€æ¨¡å‹
- LightGBMå…ƒå­¦ä¹ å™¨
- 5æŠ˜äº¤å‰éªŒè¯
- å¤šç§åŸºçº¿å¯¹æ¯”

**ä½¿ç”¨æ–¹æ³•**:
```bash
# è¿è¡ŒStackingé›†æˆ
python scripts/training/run_stacking.py

# æŒ‡å®šä¸åŒçš„åŸºç¡€æ¨¡å‹
python scripts/training/run_stacking.py --models resnet_optimized_1.12,resnet_fusion
```

**è¾“å‡º**:
- é›†æˆæ€§èƒ½å¯¹æ¯”
- 5æŠ˜CVç»“æœ
- æœ€ç»ˆStackingæ¨¡å‹

---

## ğŸ”§ è®­ç»ƒç¯å¢ƒé…ç½®

### ä¾èµ–åŒ…
```bash
pip install torch torchvision
pip install numpy scikit-learn
pip install lightgbm
pip install tqdm
```

### ç¡¬ä»¶è¦æ±‚
- **GPU**: æ¨è8GB+æ˜¾å­˜
- **å†…å­˜**: 16GB+ RAM
- **å­˜å‚¨**: 10GB+ å¯ç”¨ç©ºé—´

### æ•°æ®å‡†å¤‡
ç¡®ä¿ä»¥ä¸‹æ•°æ®æ–‡ä»¶å­˜åœ¨:
```
data/
â”œâ”€â”€ train.npz      # è®­ç»ƒé›†
â”œâ”€â”€ val.npz        # éªŒè¯é›†
â””â”€â”€ test_public.npz # æµ‹è¯•é›†
```

---

## ğŸ“Š è®­ç»ƒæµç¨‹è¯¦è§£

### 1. å•æ¨¡å‹è®­ç»ƒæµç¨‹

```python
# 1. æ•°æ®åŠ è½½
dataset = PairNPZDataset('data/train.npz', is_train=True)
dataloader = DataLoader(dataset, batch_size=64, shuffle=True)

# 2. æ¨¡å‹åˆ›å»º
model = ResNetCompareNet(feat_dim=256, layers=[2,2,2])

# 3. è®­ç»ƒé…ç½®
optimizer = AdamW(model.parameters(), lr=1e-3)
criterion = FocalLoss(alpha=1.0, gamma=2.0)
scheduler = ReduceLROnPlateau(optimizer, patience=3)

# 4. è®­ç»ƒå¾ªç¯
for epoch in range(100):
    train_one_epoch(model, dataloader, optimizer, criterion)
    val_acc = validate(model, val_dataloader)
    scheduler.step(val_acc)
    
    if val_acc > best_acc:
        best_acc = val_acc
        torch.save(model.state_dict(), 'best_model.pt')
```

### 2. å¤šç§å­è®­ç»ƒæµç¨‹

```python
# 1. å®šä¹‰ç§å­åˆ—è¡¨
seeds = [42, 2023, 2024, 2025]

# 2. ä¸ºæ¯ä¸ªç§å­è®­ç»ƒæ¨¡å‹
for seed in seeds:
    torch.manual_seed(seed)
    np.random.seed(seed)
    
    # è®­ç»ƒæ¨¡å‹
    model = train_model(seed=seed)
    
    # ä¿å­˜æ¨¡å‹
    save_model(model, f'models/seed_{seed}/')
```

### 3. Stackingé›†æˆæµç¨‹

```python
# 1. åŠ è½½æ‰€æœ‰åŸºç¡€æ¨¡å‹
models = load_all_base_models()

# 2. è·å–é¢„æµ‹ç»“æœ
X = get_predictions(models, val_data)  # [N, num_models]

# 3. è®­ç»ƒå…ƒå­¦ä¹ å™¨
lgb_model = LGBMClassifier()
lgb_model.fit(X, y)

# 4. é›†æˆé¢„æµ‹
final_pred = lgb_model.predict_proba(X)[:, 1]
```

---

## ğŸ¯ å…³é”®è®­ç»ƒæŠ€å·§

### 1. æŸå¤±å‡½æ•°é€‰æ‹©

**BCEWithLogitsLoss** (åŸºçº¿):
```python
criterion = nn.BCEWithLogitsLoss()
```

**Focal Loss** (Stage 2+):
```python
criterion = FocalLoss(alpha=1.0, gamma=2.0)
# å…³æ³¨éš¾æ ·æœ¬ï¼Œé™ä½ç®€å•æ ·æœ¬æƒé‡
```

### 2. ä¼˜åŒ–å™¨é…ç½®

**AdamW** (æ¨è):
```python
optimizer = AdamW(
    model.parameters(),
    lr=1e-3,
    weight_decay=5e-4  # L2æ­£åˆ™åŒ–
)
```

### 3. å­¦ä¹ ç‡è°ƒåº¦

**ReduceLROnPlateau**:
```python
scheduler = ReduceLROnPlateau(
    optimizer,
    mode='max',      # ç›‘æ§éªŒè¯å‡†ç¡®ç‡
    factor=0.5,      # å­¦ä¹ ç‡è¡°å‡å› å­
    patience=3       # å®¹å¿è½®æ•°
)
```

### 4. æ—©åœç­–ç•¥

```python
patience = 5
patience_counter = 0

if val_acc > best_acc:
    best_acc = val_acc
    patience_counter = 0
    torch.save(model.state_dict(), 'best_model.pt')
else:
    patience_counter += 1
    if patience_counter >= patience:
        print("Early stopping!")
        break
```

### 5. æ··åˆç²¾åº¦è®­ç»ƒ

```python
from torch.cuda.amp import autocast, GradScaler

scaler = GradScaler()

with autocast():
    logits = model(xa, xb)
    loss = criterion(logits, labels)

scaler.scale(loss).backward()
scaler.step(optimizer)
scaler.update()
```

---

## ğŸ“ˆ æ€§èƒ½ç›‘æ§

### è®­ç»ƒæŒ‡æ ‡

**å…³é”®æŒ‡æ ‡**:
- è®­ç»ƒæŸå¤± (Training Loss)
- éªŒè¯å‡†ç¡®ç‡ (Validation Accuracy)
- å­¦ä¹ ç‡ (Learning Rate)
- æ¢¯åº¦èŒƒæ•° (Gradient Norm)

**ç›‘æ§å·¥å…·**:
```python
# ä½¿ç”¨tqdmæ˜¾ç¤ºè¿›åº¦
from tqdm import tqdm

for epoch in tqdm(range(epochs), desc="Training"):
    # è®­ç»ƒä»£ç 
    pass

# ä½¿ç”¨wandbè®°å½•æŒ‡æ ‡ (å¯é€‰)
import wandb
wandb.log({"val_acc": val_acc, "train_loss": train_loss})
```

### æ¨¡å‹ä¿å­˜ç­–ç•¥

**æ£€æŸ¥ç‚¹ä¿å­˜**:
```python
# æ¯ä¸ªepochä¿å­˜
torch.save({
    'epoch': epoch,
    'model_state_dict': model.state_dict(),
    'optimizer_state_dict': optimizer.state_dict(),
    'best_val_acc': best_val_acc,
}, f'checkpoint_epoch_{epoch}.pt')

# æœ€ä½³æ¨¡å‹ä¿å­˜
if val_acc > best_val_acc:
    torch.save(model.state_dict(), 'best_model.pt')
```

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. è®­ç»ƒæœ€ä½³å•æ¨¡å‹

```bash
cd mnist-demo
python scripts/training/train_88_83_multi_seed.py \
    --layers 3,3,3 \
    --epochs 100 \
    --batch_size 64 \
    --lr 1e-3
```

### 2. è®­ç»ƒå¤šç§å­æ¨¡å‹

```bash
python scripts/training/train_multi_seed_optimized.py \
    --seeds 42,2023,2024,2025 \
    --epochs 50
```

### 3. è¿è¡ŒStackingé›†æˆ

```bash
python scripts/training/run_stacking.py
```

---

## âš ï¸ æ³¨æ„äº‹é¡¹

### 1. è·¯å¾„é…ç½®

ç¡®ä¿è„šæœ¬ä¸­çš„è·¯å¾„æ­£ç¡®:
```python
# æ•°æ®è·¯å¾„
data_path = 'data/train.npz'

# æ¨¡å‹ä¿å­˜è·¯å¾„
model_path = 'models/stage2_resnet_optimized/'
```

### 2. è®¾å¤‡é€‰æ‹©

```python
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using device: {device}")
```

### 3. å†…å­˜ç®¡ç†

```python
# æ¸…ç†GPUç¼“å­˜
torch.cuda.empty_cache()

# ä½¿ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ (å¤§æ¨¡å‹)
model.gradient_checkpointing_enable()
```

### 4. éšæœºç§å­

```python
# è®¾ç½®éšæœºç§å­ç¡®ä¿å¯å¤ç°æ€§
torch.manual_seed(42)
np.random.seed(42)
random.seed(42)
```

---

## ğŸ“š è¿›é˜¶æŠ€å·§

### 1. è¶…å‚æ•°æœç´¢

```python
# ä½¿ç”¨Optunaè¿›è¡Œè¶…å‚æ•°ä¼˜åŒ–
import optuna

def objective(trial):
    lr = trial.suggest_float('lr', 1e-4, 1e-2, log=True)
    weight_decay = trial.suggest_float('weight_decay', 1e-5, 1e-3, log=True)
    
    # è®­ç»ƒæ¨¡å‹
    model = train_model(lr=lr, weight_decay=weight_decay)
    return evaluate_model(model)

study = optuna.create_study(direction='maximize')
study.optimize(objective, n_trials=100)
```

### 2. æ¨¡å‹è’¸é¦

```python
# ä½¿ç”¨æ•™å¸ˆæ¨¡å‹æŒ‡å¯¼å­¦ç”Ÿæ¨¡å‹
teacher_model = load_teacher_model()
student_model = create_student_model()

# è’¸é¦æŸå¤±
def distillation_loss(student_logits, teacher_logits, labels, temperature=3.0):
    soft_loss = F.kl_div(
        F.log_softmax(student_logits/temperature, dim=1),
        F.softmax(teacher_logits/temperature, dim=1),
        reduction='batchmean'
    )
    hard_loss = F.cross_entropy(student_logits, labels)
    return 0.7 * soft_loss + 0.3 * hard_loss
```

### 3. æ•°æ®å¢å¼º

```python
# è‡ªå®šä¹‰æ•°æ®å¢å¼º
class CustomAugmentation:
    def __init__(self):
        self.transform = transforms.Compose([
            transforms.RandomRotation(10),
            transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),
            transforms.RandomResizedCrop(28, scale=(0.9, 1.0))
        ])
    
    def __call__(self, xa, xb):
        return self.transform(xa), self.transform(xb)
```

---

## ğŸ“ å­¦ä¹ å»ºè®®

### 1. ç†è§£æ¨¡å‹æ¶æ„

- ç ”ç©¶ `src/models/simple_compare_cnn.py` ä¸­çš„ResNetCompareNet
- ç†è§£æ®‹å·®è¿æ¥ã€æ³¨æ„åŠ›æœºåˆ¶ã€ç‰¹å¾èåˆ

### 2. åˆ†æè®­ç»ƒè¿‡ç¨‹

- è§‚å¯ŸæŸå¤±æ›²çº¿å’Œå‡†ç¡®ç‡å˜åŒ–
- ç†è§£æ—©åœå’Œå­¦ä¹ ç‡è°ƒåº¦çš„ä½œç”¨

### 3. å®éªŒä¸åŒé…ç½®

- å°è¯•ä¸åŒçš„ç½‘ç»œæ·±åº¦å’Œå®½åº¦
- æµ‹è¯•ä¸åŒçš„æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨

### 4. å¯¹æ¯”ä¸åŒæ–¹æ³•

- å•æ¨¡å‹ vs é›†æˆ
- ä¸åŒæ¶æ„çš„æ€§èƒ½å·®å¼‚

---

**è®­ç»ƒä»£ç æ˜¯ç†è§£æ¨¡å‹æ¶æ„å’Œä¼˜åŒ–ç­–ç•¥çš„å…³é”®ï¼** ğŸš€

é€šè¿‡ç ”ç©¶è¿™äº›è®­ç»ƒè„šæœ¬ï¼Œä½ å¯ä»¥æ·±å…¥äº†è§£ï¼š
- æ¨¡å‹æ˜¯å¦‚ä½•æ„å»ºçš„
- ç½‘ç»œæ˜¯å¦‚ä½•è¿æ¥çš„
- è®­ç»ƒç­–ç•¥æ˜¯å¦‚ä½•ä¼˜åŒ–çš„
- æ€§èƒ½æ˜¯å¦‚ä½•æå‡çš„
