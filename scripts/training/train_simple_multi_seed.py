#!/usr/bin/env python3
"""
ç®€åŒ–çš„å¤šç§å­è®­ç»ƒè„šæœ¬ - ç›´æ¥åœ¨ä¸»è¿›ç¨‹ä¸­è¿è¡Œ
åŸºäºresnet_optimized_1.12çš„æˆåŠŸé…ç½®
"""

import os
import sys
import json
import argparse
import numpy as np
import torch
from torch.utils.data import DataLoader
from torch.optim import AdamW
from torch.optim.lr_scheduler import CosineAnnealingLR
from torch.cuda.amp import autocast, GradScaler
from tqdm import tqdm

# æ·»åŠ è·¯å¾„
sys.path.append('resnet_optimized_1.12(1)/scripts')
sys.path.append('resnet_optimized_1.12(1)')

from utils.seed import set_seed
from utils.data import PairNPZDataset
from models.simple_compare_cnn import ResNetCompareNet, count_params

class FocalLoss(torch.nn.Module):
    """Focal LossæŸå¤±å‡½æ•°ï¼ˆå¸¦æ ‡ç­¾å¹³æ»‘ï¼‰"""
    def __init__(self, alpha=1, gamma=2, label_smoothing=0.1):
        super().__init__()
        self.alpha = alpha
        self.gamma = gamma
        self.label_smoothing = label_smoothing
        
    def forward(self, inputs, targets):
        # æ ‡ç­¾å¹³æ»‘
        if self.label_smoothing > 0:
            targets = targets * (1 - self.label_smoothing) + 0.5 * self.label_smoothing
        
        # è®¡ç®—BCEæŸå¤±
        bce_loss = torch.nn.functional.binary_cross_entropy_with_logits(
            inputs, targets, reduction='none'
        )
        
        # è®¡ç®—ptï¼ˆé¢„æµ‹æ¦‚ç‡ï¼‰
        pt = torch.exp(-bce_loss)
        
        # è®¡ç®—Focal Loss
        focal_loss = self.alpha * (1 - pt) ** self.gamma * bce_loss
        
        return focal_loss.mean()

def evaluate(model, loader, device):
    """æ¨¡å‹è¯„ä¼°å‡½æ•°"""
    model.eval()
    ys, ps = [], []
    
    with torch.no_grad():
        for batch in loader:
            xa, xb, y = batch
            xa = xa.to(device)
            xb = xb.to(device)
            
            if isinstance(y, (list, tuple)) and len(y) > 0:
                if isinstance(y[0], str):
                    # æµ‹è¯•é›†æ¨¡å¼
                    logit = model(xa, xb)
                    prob = torch.sigmoid(logit)
                    pred = (prob >= 0.5).long()
                    return 0.0, 0.0
                else:
                    y = torch.tensor(y, dtype=torch.float32)
            elif isinstance(y, str):
                return 0.0, 0.0
            else:
                if not isinstance(y, torch.Tensor):
                    y = torch.tensor(y, dtype=torch.float32)
            
            y = y.to(device).float()
            
            # å‰å‘ä¼ æ’­
            logit = model(xa, xb)
            prob = torch.sigmoid(logit)
            pred = (prob >= 0.5).long()
            
            # æ”¶é›†ç»“æœ
            ys.append(y.long().cpu().numpy())
            ps.append(pred.cpu().numpy())
    
    # åˆå¹¶ç»“æœ
    y_true = np.concatenate(ys)
    y_pred = np.concatenate(ps)
    
    # è®¡ç®—å‡†ç¡®ç‡
    acc = (y_true == y_pred).mean().item()
    
    # è®¡ç®—å®å¹³å‡F1åˆ†æ•°
    f1s = []
    for cls in [0, 1]:
        tp = np.sum((y_true==cls) & (y_pred==cls))
        fp = np.sum((y_true!=cls) & (y_pred==cls))
        fn = np.sum((y_true==cls) & (y_pred!=cls))
        
        prec = tp / (tp + fp + 1e-12)
        rec  = tp / (tp + fn + 1e-12)
        f1 = 2 * prec * rec / (prec + rec + 1e-12)
        f1s.append(f1)
    
    f1_macro = float(np.mean(f1s))
    return acc, f1_macro

def train_single_seed(seed, data_dir, output_dir, epochs=80):
    """è®­ç»ƒå•ä¸ªç§å­æ¨¡å‹"""
    print(f"\n{'='*60}")
    print(f"å¼€å§‹è®­ç»ƒç§å­ {seed}")
    print(f"è¾“å‡ºç›®å½•: {output_dir}")
    print(f"è®­ç»ƒè½®æ•°: {epochs}")
    print(f"{'='*60}")
    
    # ç¯å¢ƒè®¾ç½®
    os.makedirs(output_dir, exist_ok=True)
    set_seed(seed)
    
    # è®¾å¤‡é…ç½®
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # æ··åˆç²¾åº¦è®­ç»ƒ
    use_amp = device.type == 'cuda'
    scaler = GradScaler() if use_amp else None
    
    # æ¨¡å‹åˆå§‹åŒ– - ä½¿ç”¨resnet_optimized_1.12çš„æˆåŠŸé…ç½®
    from models.simple_compare_cnn import BasicBlock
    
    model = ResNetCompareNet(
        feat_dim=256,           # ç‰¹å¾ç»´åº¦
        layers=[3, 3, 3],       # å±‚é…ç½®
        block=BasicBlock        # ä½¿ç”¨BasicBlock
    ).to(device)
    
    n_params = int(count_params(model))
    print(f"æ¨¡å‹å‚æ•°é‡: {n_params:,}")
    print(f"ç½‘ç»œç»“æ„: ResNet-3Layer-BasicBlock [3,3,3]")
    
    # æ•°æ®åŠ è½½
    train_path = os.path.join(data_dir, "train.npz")
    val_path = os.path.join(data_dir, "val.npz")
    
    train_ds = PairNPZDataset(train_path, is_train=True)
    val_ds = PairNPZDataset(val_path, is_train=False)
    
    train_loader = DataLoader(train_ds, batch_size=128, shuffle=True, num_workers=2)
    val_loader = DataLoader(val_ds, batch_size=128, shuffle=False, num_workers=2)
    
    # ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨
    optim = AdamW(model.parameters(), lr=2e-4, weight_decay=1e-3)
    scheduler = CosineAnnealingLR(optim, T_max=epochs)
    
    # æŸå¤±å‡½æ•° - ä½¿ç”¨Focal Loss
    criterion = FocalLoss(alpha=1, gamma=2, label_smoothing=0.1)
    
    # è®­ç»ƒçŠ¶æ€
    best = {"acc": 0.0, "f1": 0.0, "epoch": -1}
    
    print(f"å¼€å§‹è®­ç»ƒ - ç§å­: {seed}, è½®æ•°: {epochs}")
    print(f"ä½¿ç”¨ Focal Loss")
    print(f"æ··åˆç²¾åº¦: {'å¯ç”¨' if use_amp else 'ç¦ç”¨'}")
    
    # è®­ç»ƒå¾ªç¯
    for epoch in range(1, epochs + 1):
        # è®­ç»ƒé˜¶æ®µ
        model.train()
        pbar = tqdm(train_loader, desc=f"Epoch {epoch}/{epochs}")
        total_loss = 0
        
        for xa, xb, y in pbar:
            xa = xa.to(device)
            xb = xb.to(device)
            y = y.to(device).float()
            
            # å‰å‘ä¼ æ’­
            if use_amp and scaler is not None:
                with autocast():
                    logit = model(xa, xb)
                    loss = criterion(logit, y)
            else:
                logit = model(xa, xb)
                loss = criterion(logit, y)
            
            # åå‘ä¼ æ’­
            optim.zero_grad()
            
            if use_amp and scaler is not None:
                scaler.scale(loss).backward()
                scaler.unscale_(optim)
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)
                scaler.step(optim)
                scaler.update()
            else:
                loss.backward()
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)
                optim.step()
            
            total_loss += loss.item()
            pbar.set_postfix(loss=float(loss.item()))
        
        # å­¦ä¹ ç‡è°ƒåº¦
        scheduler.step()
        
        # éªŒè¯é˜¶æ®µ
        acc, f1 = evaluate(model, val_loader, device)
        avg_loss = total_loss / len(train_loader)
        
        print(f"[Val] epoch={epoch} acc={acc:.4f} f1_macro={f1:.4f} "
              f"loss={avg_loss:.4f} lr={optim.param_groups[0]['lr']:.6f}")
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if acc > best["acc"]:
            best = {"acc": acc, "f1": f1, "epoch": epoch}
            torch.save(model.state_dict(), os.path.join(output_dir, "model.pt"))
            
            # ä¿å­˜æŒ‡æ ‡
            with open(os.path.join(output_dir, "metrics.json"), "w") as f:
                json.dump({
                    "best_val_acc": acc,
                    "best_val_f1": f1,
                    "best_epoch": epoch,
                    "params": n_params,
                    "final_loss": avg_loss,
                    "layers": [3, 3, 3],
                    "use_bottleneck": False,
                    "feat_dim": 256,
                    "use_amp": use_amp,
                    "device": str(device),
                    "seed": seed
                }, f, indent=2)
        
        # ä¿å­˜checkpoint
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optim.state_dict(),
            'scheduler_state_dict': scheduler.state_dict(),
            'best_acc': best["acc"],
            'best_f1': best["f1"],
            'best_epoch': best["epoch"],
            'seed': seed
        }
        torch.save(checkpoint, os.path.join(output_dir, "checkpoint.pt"))
    
    # è®­ç»ƒå®Œæˆ
    print(f"ç§å­ {seed} è®­ç»ƒå®Œæˆ!")
    print(f"æœ€ä½³ç»“æœ @ epoch {best['epoch']}: acc={best['acc']:.4f}, f1={best['f1']:.4f}")
    
    return best["acc"], best["f1"]

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="ç®€åŒ–çš„å¤šç§å­è®­ç»ƒè„šæœ¬")
    parser.add_argument("--data_dir", type=str, default="resnet_optimized_1.12(1)/data", 
                       help="æ•°æ®ç›®å½•")
    parser.add_argument("--output_dir", type=str, default="multi_seed_models", 
                       help="è¾“å‡ºç›®å½•")
    parser.add_argument("--seeds", type=int, nargs='+', default=[42, 2023, 2024, 2025], 
                       help="éšæœºç§å­åˆ—è¡¨")
    parser.add_argument("--epochs", type=int, default=80, help="è®­ç»ƒè½®æ•°")
    args = parser.parse_args()
    
    print("åŸºäºresnet_optimized_1.12çš„å¤šç§å­è®­ç»ƒ")
    print("="*60)
    print(f"æ•°æ®ç›®å½•: {args.data_dir}")
    print(f"è¾“å‡ºç›®å½•: {args.output_dir}")
    print(f"ç§å­åˆ—è¡¨: {args.seeds}")
    print(f"è®­ç»ƒè½®æ•°: {args.epochs}")
    print("="*60)
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(args.output_dir, exist_ok=True)
    
    # è®­ç»ƒç»“æœ
    results = {}
    
    # ä¸²è¡Œè®­ç»ƒ
    for i, seed in enumerate(args.seeds, 1):
        print(f"\nè¿›åº¦: {i}/{len(args.seeds)}")
        
        # æ£€æŸ¥æ˜¯å¦å·²ç»è®­ç»ƒè¿‡
        output_dir = os.path.join(args.output_dir, f"seed_{seed}")
        metrics_path = os.path.join(output_dir, "metrics.json")
        
        if os.path.exists(metrics_path):
            print(f"ç§å­ {seed} å·²å­˜åœ¨ï¼Œè·³è¿‡è®­ç»ƒ")
            with open(metrics_path, 'r') as f:
                metrics = json.load(f)
            acc = metrics.get('best_val_acc', 0)
            f1 = metrics.get('best_val_f1', 0)
            results[seed] = (acc, f1)
            continue
        
        # è®­ç»ƒå•ä¸ªç§å­
        acc, f1 = train_single_seed(seed, args.data_dir, output_dir, args.epochs)
        results[seed] = (acc, f1)
    
    # è¾“å‡ºæ€»ç»“
    print(f"\n{'='*60}")
    print("è®­ç»ƒå®Œæˆæ€»ç»“")
    print(f"{'='*60}")
    
    if results:
        print("å„ç§å­ç»“æœ:")
        for seed, (acc, f1) in results.items():
            print(f"  ç§å­ {seed}: acc={acc:.4f}, f1={f1:.4f}")
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        accuracies = [acc for acc, f1 in results.values() if acc > 0]
        if accuracies:
            best_acc = max(accuracies)
            avg_acc = sum(accuracies) / len(accuracies)
            best_seed = max(results.keys(), key=lambda s: results[s][0])
            
            print(f"\nç»Ÿè®¡ä¿¡æ¯:")
            print(f"  æœ€ä½³å‡†ç¡®ç‡: {best_acc:.4f} (ç§å­ {best_seed})")
            print(f"  å¹³å‡å‡†ç¡®ç‡: {avg_acc:.4f}")
            print(f"  æˆåŠŸè®­ç»ƒ: {len(accuracies)}/{len(args.seeds)} ä¸ªç§å­")
            
            # ä¿å­˜æ€»ç»“
            summary = {
                "seeds": args.seeds,
                "epochs": args.epochs,
                "results": {str(seed): {"acc": acc, "f1": f1} for seed, (acc, f1) in results.items()},
                "best_accuracy": best_acc,
                "best_seed": best_seed,
                "average_accuracy": avg_acc,
                "successful_trains": len(accuracies)
            }
            
            summary_path = os.path.join(args.output_dir, "training_summary.json")
            with open(summary_path, 'w') as f:
                json.dump(summary, f, indent=2)
            
            print(f"\næ€»ç»“å·²ä¿å­˜åˆ°: {summary_path}")
            
            # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°90%
            if best_acc >= 0.90:
                print(f"ğŸ‰ æ­å–œï¼å·²è¾¾åˆ°90%ç›®æ ‡ï¼")
            else:
                gap = 0.90 - best_acc
                print(f"è·ç¦»90%ç›®æ ‡è¿˜å·®: {gap:.4f} ({gap*100:.2f}%)")
    else:
        print("æ²¡æœ‰æˆåŠŸè®­ç»ƒçš„æ¨¡å‹")
    
    print(f"\nä¸‹ä¸€æ­¥å»ºè®®:")
    print(f"1. æ£€æŸ¥è®­ç»ƒæ—¥å¿—ï¼Œç¡®ä¿æ‰€æœ‰æ¨¡å‹éƒ½è®­ç»ƒå®Œæˆ")
    print(f"2. è¿è¡Œé›†æˆæµ‹è¯•: python final_ensemble.py")
    print(f"3. å¦‚æœæœªè¾¾åˆ°90%ï¼Œè€ƒè™‘è®­ç»ƒæ›´å¤šç§å­æˆ–è°ƒæ•´è¶…å‚æ•°")

if __name__ == "__main__":
    main()
