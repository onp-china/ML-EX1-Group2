# MNIST Digit Pair Comparison - Stacking Ensemble Solution

This project implements a **Stacking Ensemble** approach for binary classification of MNIST digit pairs, achieving **90.81% accuracy on validation set** and **89.85% on test_public set**.

## ðŸ“Š Final Model Performance

### Performance Summary

| Model | Train Accuracy | Val Accuracy | Test Public Accuracy | Parameters |
|-------|---------------|--------------|---------------------|------------|
| resnet_optimized_1.12 | 96.02% | 88.74% | 88.45% | ~5M |
| resnet_fusion | 93.42% | 87.92% | 89.00% | ~5M |
| resnet_optimized | 91.37% | 87.80% | 88.20% | ~5M |
| seed_2025 | 95.00% | 87.39% | 87.85% | ~5M |
| seed_2023 | 94.72% | 87.02% | 87.95% | ~5M |
| seed_2024 | 93.77% | 86.71% | 86.75% | ~5M |
| fpn_model | 95.60% | 87.30% | 87.90% | 5,089,626 |
| resnet_fusion_seed42 | 89.74% | 85.46% | 85.25% | ~5M |
| resnet_fusion_seed456 | 88.76% | 84.88% | 83.50% | ~5M |
| resnet_fusion_seed123 | 88.33% | 84.56% | 84.80% | ~5M |
| **Stacking Ensemble** | **95.67%** | **90.81%** | **89.85%** | 10 models + LightGBM |

### Model Size and Training Time

- **Individual Model Size**: ~20 MB per model (PyTorch .pt file)
- **Total Ensemble Size**: ~200 MB (10 models)
- **Meta-learner Size**: <1 MB (LightGBM)
- **Training Time per Model**: 30-60 minutes on NVIDIA GPU (varies by architecture)
- **Total Training Time**: ~8-10 hours for all 10 models
- **Stacking Meta-learner Training**: ~2 minutes
- **Inference Time**: ~5 seconds for 8,000 samples on GPU

## ðŸ—ï¸ Project Structure

```
mnist-demo/
â”œâ”€â”€ src/                          # Source code
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ data_loader.py           # Data loading utilities
â”‚   â”œâ”€â”€ model_loader.py          # Model loading utilities
â”‚   â”œâ”€â”€ models/                  # Model architectures
â”‚   â”‚   â”œâ”€â”€ simple_compare_cnn.py    # ResNet-based models
â”‚   â”‚   â””â”€â”€ fpn_architecture_v2.py   # FPN model
â”‚   â””â”€â”€ stacking_ensemble.py     # Stacking ensemble implementation
â”œâ”€â”€ models/                       # Trained model weights
â”‚   â”œâ”€â”€ stage2_resnet_optimized/
â”‚   â”‚   â”œâ”€â”€ resnet_optimized_1.12/
â”‚   â”‚   â”œâ”€â”€ resnet_fusion/
â”‚   â”‚   â””â”€â”€ resnet_optimized/
â”‚   â””â”€â”€ stage3_multi_seed/
â”‚       â”œâ”€â”€ seed_2023/, seed_2024/, seed_2025/
â”‚       â”œâ”€â”€ fpn_model/
â”‚       â””â”€â”€ resnet_fusion_seed*/
â”œâ”€â”€ data/                         # Dataset files
â”‚   â”œâ”€â”€ train.npz
â”‚   â”œâ”€â”€ val.npz
â”‚   â”œâ”€â”€ test_public.npz
â”‚   â”œâ”€â”€ test_public_labels.csv
â”‚   â””â”€â”€ test_private.npz
â”œâ”€â”€ scripts/                      # Training and evaluation scripts
â”‚   â”œâ”€â”€ training/                # Training scripts
â”‚   â”œâ”€â”€ generate_private_predictions.py
â”‚   â”œâ”€â”€ evaluate_10models_stacking.py
â”‚   â””â”€â”€ create_final_visualizations.py
â”œâ”€â”€ outputs/                      # Results and visualizations
â”‚   â”œâ”€â”€ results/
â”‚   â”‚   â”œâ”€â”€ performance_table.csv
â”‚   â”‚   â””â”€â”€ 10models_stacking_evaluation.json
â”‚   â””â”€â”€ visualizations/
â”‚       â”œâ”€â”€ performance_table.png
â”‚       â”œâ”€â”€ confusion_matrix.png
â”‚       â””â”€â”€ stacking_training_process.png
â”œâ”€â”€ requirements.txt              # Python dependencies
â”œâ”€â”€ pred_private.csv             # Final predictions for test_private
â”œâ”€â”€ check_submission.py          # Submission format validator
â””â”€â”€ README.md                    # This file
```

## ðŸš€ Quick Start

### 1. Environment Setup

#### Option A: Using pip

```bash
# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

#### Option B: Using conda

```bash
# Create conda environment
conda create -n mnist-demo python=3.10
conda activate mnist-demo

# Install dependencies
pip install -r requirements.txt
```

### Required Dependencies

- Python 3.8+
- PyTorch 2.0+ (with CUDA support recommended)
- NumPy
- Pandas
- scikit-learn
- LightGBM
- Matplotlib
- Seaborn

### 2. Data Preparation

Place the following files in the `data/` directory:
- `train.npz` (50,000 samples)
- `val.npz` (10,000 samples)
- `test_public.npz` (2,000 samples)
- `test_public_labels.csv`
- `test_private.npz` (8,000 samples)

## ðŸ“ˆ Model Training

### Training Individual Models

The project uses 10 pre-trained models. To retrain from scratch:

```bash
# Train ResNet-based models (Stage 2)
python scripts/training/train_stage2_resnet_optimized.py

# Train multi-seed models (Stage 3)
python scripts/training/train_stage3_multi_seed.py
```

**Note**: Training all models from scratch takes approximately 8-10 hours on a modern GPU.

### Training Stacking Meta-learner

The stacking meta-learner is trained automatically during prediction generation:

```bash
python scripts/generate_private_predictions.py
```

This script:
1. Loads all 10 pre-trained models
2. Generates predictions on validation set
3. Trains LightGBM meta-learner using 5-fold cross-validation
4. Generates final predictions for `test_private.npz`

## ðŸ”® Generating Predictions

### Generate Predictions for test_private.npz

```bash
python scripts/generate_private_predictions.py
```

This creates `pred_private.csv` with predictions for the private test set.

### Validate Submission Format

```bash
python check_submission.py --data_dir data --pred pred_private.csv --test_file test_private.npz
```

Expected output:
```
OK: id coverage matches the test set.
CSV format looks good.
```

## ðŸ“Š Model Evaluation

### Evaluate on Validation and Test Public Sets

```bash
python scripts/evaluate_10models_stacking.py
```

This generates:
- Cross-validation scores (5-fold)
- Performance on validation set
- Performance on test_public set
- Detailed metrics in `outputs/results/10models_stacking_evaluation.json`

### Create Visualizations

```bash
python scripts/create_final_visualizations.py
```

This generates:
1. **Performance Table** (`outputs/visualizations/performance_table.png`)
   - Comparison of all models across train/val/test_public
   
2. **Confusion Matrix** (`outputs/visualizations/confusion_matrix.png`)
   - Stacking ensemble performance on validation set
   
3. **Stacking Training Process** (`outputs/visualizations/stacking_training_process.png`)
   - Cross-validation scores
   - Comparison with simple averaging

## ðŸ§  Model Architecture

### Base Models

The ensemble consists of 10 ResNet-based and FPN-based models:

1. **ResNet Variants** (9 models)
   - Architecture: Modified ResNet with BasicBlock
   - Layers: [2, 2, 2, 2] or [2, 2, 2]
   - Feature dimension: 256
   - Width multiplier: 1.0 or 1.12
   - Input: Two 28Ã—28 grayscale images
   - Output: Binary classification (same digit or not)

2. **FPN Model** (1 model)
   - Architecture: Feature Pyramid Network
   - Layers: [2, 2, 2]
   - Feature dimension: 256
   - Width multiplier: 1.25
   - Multi-scale feature extraction

### Stacking Meta-learner

- **Algorithm**: LightGBM Classifier
- **Input**: Concatenated predictions from 10 base models (20 features)
- **Training**: 5-fold stratified cross-validation on validation set
- **Hyperparameters**:
  - `n_estimators`: 100
  - `learning_rate`: 0.05
  - `max_depth`: 5
  - `num_leaves`: 31

### Training Strategy

1. **Data Augmentation**: Random rotation, scaling, translation
2. **Loss Function**: Focal Loss with label smoothing
3. **Optimizer**: AdamW with weight decay
4. **Learning Rate Scheduler**: ReduceLROnPlateau
5. **Early Stopping**: Patience of 10 epochs
6. **Multi-seed Training**: Seeds 42, 2023, 2024, 2025 for robustness

## ðŸ“ Output Files

### pred_private.csv

Format:
```csv
id,label
PRV_0000000,1
PRV_0000001,0
...
```

- **Rows**: 8,001 (including header)
- **Columns**: `id`, `label`
- **Label Distribution**: ~50% class 0, ~50% class 1

### Performance Metrics

Located in `outputs/results/`:
- `performance_table.csv`: Detailed performance comparison
- `10models_stacking_evaluation.json`: Full evaluation results
- `10models_final_report.md`: Comprehensive report

### Visualizations

Located in `outputs/visualizations/`:
- `performance_table.png`: Performance comparison table
- `confusion_matrix.png`: Confusion matrix on validation set
- `stacking_training_process.png`: Training process visualization

## ðŸŽ¯ Key Results

### Stacking Ensemble Performance

- **Validation Set**: 90.81% accuracy (F1: 90.78%)
- **Test Public Set**: 89.85% accuracy (F1: 89.92%)
- **Improvement over Simple Average**: +2.21% on validation, +0.95% on test_public

### Cross-Validation Results

| Fold | Accuracy |
|------|----------|
| Fold 1 | 89.25% |
| Fold 2 | 90.75% |
| Fold 3 | 88.95% |
| Fold 4 | 90.25% |
| Fold 5 | 87.20% |
| **Mean** | **89.28% Â± 1.23%** |

### Confusion Matrix (Validation Set)

|  | Predicted 0 | Predicted 1 |
|---|------------|------------|
| **True 0** | 4,557 | 443 |
| **True 1** | 476 | 4,524 |

**Accuracy**: 90.81%

## ðŸ”¬ Technical Details

### Why Stacking Works

1. **Model Diversity**: 10 models with different architectures, seeds, and hyperparameters
2. **Meta-learning**: LightGBM learns optimal weights for each model's predictions
3. **Ensemble Strength**: Combines complementary strengths of individual models
4. **Robustness**: Multi-seed training reduces variance

### Design Decisions

1. **10 Models**: Balance between performance and computational cost
2. **LightGBM**: Fast, accurate, handles feature interactions well
3. **5-Fold CV**: Ensures meta-learner doesn't overfit to validation set
4. **Focal Loss**: Addresses class imbalance and hard examples

## ðŸ“ Citation

If you use this code, please cite:

```
MNIST Digit Pair Comparison - Stacking Ensemble Solution
10-Model Ensemble with LightGBM Meta-learner
Validation Accuracy: 90.81% | Test Public Accuracy: 89.85%
```

## ðŸ“„ License

This project is for educational and research purposes.

## ðŸ™ Acknowledgments

- MNIST dataset creators
- PyTorch and scikit-learn communities
- LightGBM developers

---

**Last Updated**: October 26, 2025

**Contact**: For questions or issues, please refer to the project documentation.
