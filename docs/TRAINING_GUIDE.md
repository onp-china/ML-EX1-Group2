# è®­ç»ƒæŒ‡å—

> **ä»é›¶å¼€å§‹è®­ç»ƒæ‰€æœ‰æ¨¡å‹ - å®Œæ•´å¤ç°æŒ‡å—**

æœ¬æŒ‡å—å°†å¸¦ä½ ä»é›¶å¼€å§‹è®­ç»ƒæ‰€æœ‰é˜¶æ®µçš„æ¨¡å‹ï¼Œå®Œæ•´å¤ç°ä»57%åˆ°93%çš„ä¼˜åŒ–å†ç¨‹ã€‚

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. ç¯å¢ƒå‡†å¤‡

```bash
# å®‰è£…ä¾èµ–
pip install -r requirements.txt

# éªŒè¯ç¯å¢ƒ
python -c "import torch; print('PyTorchç‰ˆæœ¬:', torch.__version__)"
python -c "import lightgbm; print('LightGBMç‰ˆæœ¬:', lightgbm.__version__)"
```

### 2. æ•°æ®å‡†å¤‡

ç¡®ä¿æ•°æ®æ–‡ä»¶å­˜åœ¨:
```bash
ls data/
# åº”è¯¥çœ‹åˆ°: train.npz, val.npz, test_public.npz
```

### 3. ä¸€é”®è®­ç»ƒæ‰€æœ‰æ¨¡å‹

```bash
# è®­ç»ƒæ‰€æœ‰é˜¶æ®µæ¨¡å‹
bash train_all_stages.sh

# æˆ–è€…åˆ†é˜¶æ®µè®­ç»ƒ
python scripts/training/train_stage1_improvedv2.py
python scripts/training/train_stage2_resnet_optimized.py --layers 3,3,3
python scripts/training/train_stage3_multi_seed.py --seeds 42,2023,2024,2025
python scripts/training/run_stacking.py
```

---

## ğŸ“Š åˆ†é˜¶æ®µè®­ç»ƒè¯¦è§£

### Stage 1: ç‰¹å¾èåˆé©å‘½ (57%â†’85%)

**ç›®æ ‡**: å®ç°6å¤´èåˆ + CBAMæ³¨æ„åŠ›æœºåˆ¶

```bash
python scripts/training/train_stage1_improvedv2.py \
    --epochs 50 \
    --batch_size 64 \
    --lr 1e-3 \
    --feat_dim 256 \
    --layers 2,2,2
```

**å…³é”®åˆ›æ–°**:
- 6ç§ç‰¹å¾èåˆæ–¹å¼ (å·®å€¼ã€æ‹¼æ¥ã€ä¹˜ç§¯ã€ä½™å¼¦ç›¸ä¼¼åº¦ã€L2è·ç¦»ã€æ®‹å·®)
- è‡ªé€‚åº”åŠ æƒèåˆ
- CBAMæ³¨æ„åŠ›æœºåˆ¶

**é¢„æœŸç»“æœ**: 85.28% éªŒè¯å‡†ç¡®ç‡

---

### Stage 2: æ·±åº¦ä¼˜åŒ–çªç ´ (85%â†’89%)

**ç›®æ ‡**: å®ç°ResNet + Focal Loss + æ··åˆç²¾åº¦è®­ç»ƒ

#### 2.1 è®­ç»ƒæœ€ä½³å•æ¨¡å‹ (ResNet-Optimized-1.12)

```bash
python scripts/training/train_stage2_resnet_optimized.py \
    --model_name resnet_optimized_1.12 \
    --layers 3,3,3 \
    --epochs 100 \
    --batch_size 64 \
    --lr 1e-3 \
    --width_mult 1.0
```

**å…³é”®ç‰¹æ€§**:
- ResNet[3,3,3] æ·±åº¦ç½‘ç»œ
- Focal Loss (Î±=1.0, Î³=2.0)
- æ··åˆç²¾åº¦è®­ç»ƒ (AMP)
- SE-Moduleé€šé“æ³¨æ„åŠ›

**é¢„æœŸç»“æœ**: 88.75% éªŒè¯å‡†ç¡®ç‡

#### 2.2 è®­ç»ƒResNet-Fusion

```bash
python scripts/training/train_stage2_resnet_optimized.py \
    --model_name resnet_fusion \
    --layers 2,2,2 \
    --use_fusion \
    --epochs 80
```

**é¢„æœŸç»“æœ**: 87.92% éªŒè¯å‡†ç¡®ç‡

#### 2.3 è®­ç»ƒResNet-Optimized

```bash
python scripts/training/train_stage2_resnet_optimized.py \
    --model_name resnet_optimized \
    --layers 2,2,2 \
    --epochs 80
```

**é¢„æœŸç»“æœ**: 87.80% éªŒè¯å‡†ç¡®ç‡

---

### Stage 3: å¤šæ ·æ€§æ¢ç´¢ (84%â†’88%)

**ç›®æ ‡**: å®ç°å¤šç§å­è®­ç»ƒ + æ¶æ„å¤šæ ·æ€§

#### 3.1 è®­ç»ƒå¤šç§å­ResNetæ¨¡å‹

```bash
python scripts/training/train_stage3_multi_seed.py \
    --seeds 42,2023,2024,2025 \
    --epochs 50 \
    --batch_size 64 \
    --layers 2,2,2 \
    --width_mult 1.0
```

**é¢„æœŸç»“æœ**:
- Seed 2025: 87.39%
- Seed 2023: 87.02%
- Seed 2024: 86.71%
- Seed 42: 85.38%

#### 3.2 è®­ç»ƒFPNå¤šå°ºåº¦æ¨¡å‹

```bash
python scripts/training/train_stage3_multi_seed.py \
    --seeds 42 \
    --architecture fpn \
    --epochs 50 \
    --layers 2,2,2 \
    --width_mult 1.0
```

**é¢„æœŸç»“æœ**: 87.30% éªŒè¯å‡†ç¡®ç‡

#### 3.3 è®­ç»ƒFusionå¤šç§å­æ¨¡å‹

```bash
python scripts/training/train_stage3_multi_seed.py \
    --seeds 42,123,456 \
    --epochs 50 \
    --layers 2,2,2 \
    --use_fusion
```

**é¢„æœŸç»“æœ**:
- Seed 42: 85.38%
- Seed 123: 84.36%
- Seed 456: 84.39%

---

### Stage 4: Stackingé›†æˆ (89%â†’93%)

**ç›®æ ‡**: å®ç°LightGBMå…ƒå­¦ä¹ å™¨é›†æˆ

```bash
python scripts/training/run_stacking.py
```

**å…³é”®ç‰¹æ€§**:
- 10ä¸ªå¼‚æ„åŸºç¡€æ¨¡å‹
- LightGBMå…ƒå­¦ä¹ å™¨
- 5æŠ˜äº¤å‰éªŒè¯
- å¤šç§åŸºçº¿å¯¹æ¯”

**é¢„æœŸç»“æœ**: 93.09% éªŒè¯å‡†ç¡®ç‡

---

## ğŸ”§ è®­ç»ƒé…ç½®è¯¦è§£

### é€šç”¨å‚æ•°

| å‚æ•° | é»˜è®¤å€¼ | è¯´æ˜ |
|------|--------|------|
| `--epochs` | 50-100 | è®­ç»ƒè½®æ•° |
| `--batch_size` | 64 | æ‰¹æ¬¡å¤§å° |
| `--lr` | 1e-3 | å­¦ä¹ ç‡ |
| `--weight_decay` | 5e-4 | æƒé‡è¡°å‡ |
| `--patience` | 5-10 | æ—©åœå®¹å¿åº¦ |

### æ¨¡å‹å‚æ•°

| å‚æ•° | é»˜è®¤å€¼ | è¯´æ˜ |
|------|--------|------|
| `--feat_dim` | 256 | ç‰¹å¾ç»´åº¦ |
| `--layers` | 2,2,2 | ResNetå±‚æ•° |
| `--width_mult` | 1.0 | å®½åº¦å€æ•° |
| `--use_bottleneck` | False | ä½¿ç”¨Bottleneckå— |
| `--use_fusion` | False | ä½¿ç”¨5å¤´èåˆ |

### è®­ç»ƒç­–ç•¥

| ç­–ç•¥ | è¯´æ˜ | æ•ˆæœ |
|------|------|------|
| **Focal Loss** | å…³æ³¨éš¾æ ·æœ¬ | +0.8% |
| **æ··åˆç²¾åº¦** | åŠ é€Ÿè®­ç»ƒ | 2xé€Ÿåº¦ |
| **æ¢¯åº¦è£å‰ª** | ç¨³å®šè®­ç»ƒ | é¿å…æ¢¯åº¦çˆ†ç‚¸ |
| **æ ‡ç­¾å¹³æ»‘** | æ­£åˆ™åŒ– | +0.2% |
| **æ—©åœ** | é˜²æ­¢è¿‡æ‹Ÿåˆ | èŠ‚çœæ—¶é—´ |

---

## ğŸ“ˆ è®­ç»ƒç›‘æ§

### å®æ—¶ç›‘æ§

```bash
# ä½¿ç”¨tensorboardç›‘æ§è®­ç»ƒè¿‡ç¨‹
tensorboard --logdir=outputs/logs

# æˆ–è€…æŸ¥çœ‹è®­ç»ƒæ—¥å¿—
tail -f outputs/logs/training.log
```

### å…³é”®æŒ‡æ ‡

**è®­ç»ƒæŒ‡æ ‡**:
- è®­ç»ƒæŸå¤± (Training Loss)
- éªŒè¯å‡†ç¡®ç‡ (Validation Accuracy)
- å­¦ä¹ ç‡ (Learning Rate)
- æ¢¯åº¦èŒƒæ•° (Gradient Norm)

**æ€§èƒ½æŒ‡æ ‡**:
- å‡†ç¡®ç‡ (Accuracy)
- F1åˆ†æ•° (F1-Score)
- ç²¾ç¡®ç‡ (Precision)
- å¬å›ç‡ (Recall)

### æ¨¡å‹ä¿å­˜

**è‡ªåŠ¨ä¿å­˜**:
- æœ€ä½³æ¨¡å‹: `models/stage*/model_name/model.pt`
- é…ç½®ä¿¡æ¯: `models/stage*/model_name/metrics.json`
- è®­ç»ƒæ—¥å¿—: `outputs/logs/training.log`

**æ‰‹åŠ¨ä¿å­˜**:
```python
# ä¿å­˜æ£€æŸ¥ç‚¹
torch.save({
    'epoch': epoch,
    'model_state_dict': model.state_dict(),
    'optimizer_state_dict': optimizer.state_dict(),
    'best_val_acc': best_val_acc,
}, 'checkpoint.pt')

# åŠ è½½æ£€æŸ¥ç‚¹
checkpoint = torch.load('checkpoint.pt')
model.load_state_dict(checkpoint['model_state_dict'])
```

---

## âš¡ æ€§èƒ½ä¼˜åŒ–

### 1. ç¡¬ä»¶ä¼˜åŒ–

**GPUé…ç½®**:
```bash
# è®¾ç½®GPUå†…å­˜å¢é•¿
export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512

# ä½¿ç”¨å¤šGPUè®­ç»ƒ
python -m torch.distributed.launch --nproc_per_node=2 train_stage2_resnet_optimized.py
```

**å†…å­˜ä¼˜åŒ–**:
```python
# æ¢¯åº¦æ£€æŸ¥ç‚¹
model.gradient_checkpointing_enable()

# æ¸…ç†GPUç¼“å­˜
torch.cuda.empty_cache()
```

### 2. è®­ç»ƒä¼˜åŒ–

**æ··åˆç²¾åº¦è®­ç»ƒ**:
```python
from torch.cuda.amp import autocast, GradScaler

scaler = GradScaler()

with autocast():
    logits = model(xa, xb)
    loss = criterion(logits, labels)

scaler.scale(loss).backward()
scaler.step(optimizer)
scaler.update()
```

**æ•°æ®å¹¶è¡Œ**:
```python
# å•æœºå¤šGPU
model = nn.DataParallel(model)

# åˆ†å¸ƒå¼è®­ç»ƒ
model = nn.parallel.DistributedDataParallel(model)
```

### 3. è¶…å‚æ•°ä¼˜åŒ–

**ç½‘æ ¼æœç´¢**:
```python
# å­¦ä¹ ç‡æœç´¢
lrs = [1e-4, 5e-4, 1e-3, 5e-3, 1e-2]
for lr in lrs:
    train_model(lr=lr)
```

**è´å¶æ–¯ä¼˜åŒ–**:
```python
import optuna

def objective(trial):
    lr = trial.suggest_float('lr', 1e-4, 1e-2, log=True)
    weight_decay = trial.suggest_float('weight_decay', 1e-5, 1e-3, log=True)
    return train_model(lr=lr, weight_decay=weight_decay)

study = optuna.create_study(direction='maximize')
study.optimize(objective, n_trials=100)
```

---

## ğŸ› å¸¸è§é—®é¢˜

### 1. å†…å­˜ä¸è¶³

**é—®é¢˜**: `CUDA out of memory`

**è§£å†³æ–¹æ¡ˆ**:
```bash
# å‡å°æ‰¹æ¬¡å¤§å°
python train_stage2_resnet_optimized.py --batch_size 32

# ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯
python train_stage2_resnet_optimized.py --gradient_accumulation_steps 2

# æ¸…ç†GPUç¼“å­˜
python -c "import torch; torch.cuda.empty_cache()"
```

### 2. è®­ç»ƒä¸æ”¶æ•›

**é—®é¢˜**: æŸå¤±ä¸ä¸‹é™æˆ–å‡†ç¡®ç‡ä¸æå‡

**è§£å†³æ–¹æ¡ˆ**:
```bash
# é™ä½å­¦ä¹ ç‡
python train_stage2_resnet_optimized.py --lr 1e-4

# å¢åŠ æƒé‡è¡°å‡
python train_stage2_resnet_optimized.py --weight_decay 1e-3

# æ£€æŸ¥æ•°æ®åŠ è½½
python -c "from data_loader import PairNPZDataset; print(len(PairNPZDataset('data/train.npz')))"
```

### 3. æ¨¡å‹æ€§èƒ½å·®

**é—®é¢˜**: å‡†ç¡®ç‡è¿œä½äºé¢„æœŸ

**è§£å†³æ–¹æ¡ˆ**:
```bash
# æ£€æŸ¥æ•°æ®é¢„å¤„ç†
python -c "from data_loader import PairNPZDataset; dataset = PairNPZDataset('data/train.npz'); print(dataset[0])"

# æ£€æŸ¥æ¨¡å‹æ¶æ„
python -c "from models.simple_compare_cnn import ResNetCompareNet; model = ResNetCompareNet(); print(model)"

# æ£€æŸ¥æŸå¤±å‡½æ•°
python -c "from torch.nn import BCEWithLogitsLoss; criterion = BCEWithLogitsLoss(); print(criterion)"
```

### 4. è®­ç»ƒé€Ÿåº¦æ…¢

**é—®é¢˜**: è®­ç»ƒæ—¶é—´è¿‡é•¿

**è§£å†³æ–¹æ¡ˆ**:
```bash
# ä½¿ç”¨æ··åˆç²¾åº¦
python train_stage2_resnet_optimized.py --use_amp

# å¢åŠ æ‰¹æ¬¡å¤§å°
python train_stage2_resnet_optimized.py --batch_size 128

# ä½¿ç”¨å¤šè¿›ç¨‹æ•°æ®åŠ è½½
python train_stage2_resnet_optimized.py --num_workers 4
```

---

## ğŸ“Š è®­ç»ƒç»“æœéªŒè¯

### 1. å•æ¨¡å‹éªŒè¯

```bash
# æµ‹è¯•å•ä¸ªæ¨¡å‹
python scripts/test_single_model.py --model resnet_optimized_1.12

# é¢„æœŸè¾“å‡º
# å‡†ç¡®ç‡ (Accuracy):  0.8875 (88.75%)
# F1åˆ†æ•° (F1-Score):  0.8872
```

### 2. å…¨æ¨¡å‹å¯¹æ¯”

```bash
# æµ‹è¯•æ‰€æœ‰æ¨¡å‹
python scripts/test_all_models.py

# é¢„æœŸè¾“å‡º
# ResNet-Optimized-1.12: 88.75%
# ResNet-Fusion: 87.92%
# Multi-Seed 2025: 87.39%
# ...
```

### 3. Stackingé›†æˆéªŒè¯

```bash
# è¿è¡ŒStackingé›†æˆ
python scripts/training/run_stacking.py

# é¢„æœŸè¾“å‡º
# Stacking final accuracy: 0.9309 (93.09%)
```

---

## ğŸ¯ è®­ç»ƒç›®æ ‡

### æ€§èƒ½ç›®æ ‡

| é˜¶æ®µ | ç›®æ ‡å‡†ç¡®ç‡ | å®é™…è¾¾åˆ° | çŠ¶æ€ |
|------|-----------|---------|------|
| Stage 1 | 85% | 85.28% | âœ… |
| Stage 2 | 88% | 88.75% | âœ… |
| Stage 3 | 87% | 87.39% | âœ… |
| Stage 4 | 90% | 93.09% | âœ… |

### æ—¶é—´ç›®æ ‡

| æ¨¡å‹ | é¢„æœŸæ—¶é—´ | ç¡¬ä»¶è¦æ±‚ |
|------|---------|---------|
| ImprovedV2 | 30åˆ†é’Ÿ | 8GB GPU |
| ResNet-Opt | 2å°æ—¶ | 8GB GPU |
| Multi-Seed | 4å°æ—¶ | 8GB GPU |
| Stacking | 10åˆ†é’Ÿ | 8GB GPU |

---

## ğŸš€ è¿›é˜¶æŠ€å·§

### 1. è‡ªå®šä¹‰æŸå¤±å‡½æ•°

```python
class CustomLoss(nn.Module):
    def __init__(self, alpha=1.0, beta=0.5):
        super().__init__()
        self.alpha = alpha
        self.beta = beta
        
    def forward(self, inputs, targets):
        bce_loss = F.binary_cross_entropy_with_logits(inputs, targets)
        focal_loss = self.alpha * (1 - torch.sigmoid(inputs)) ** 2 * bce_loss
        return focal_loss
```

### 2. è‡ªå®šä¹‰æ•°æ®å¢å¼º

```python
class CustomAugmentation:
    def __init__(self):
        self.transform = transforms.Compose([
            transforms.RandomRotation(15),
            transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),
            transforms.RandomResizedCrop(28, scale=(0.9, 1.0))
        ])
    
    def __call__(self, xa, xb):
        return self.transform(xa), self.transform(xb)
```

### 3. æ¨¡å‹è’¸é¦

```python
def distillation_loss(student_logits, teacher_logits, labels, temperature=3.0):
    soft_loss = F.kl_div(
        F.log_softmax(student_logits/temperature, dim=1),
        F.softmax(teacher_logits/temperature, dim=1),
        reduction='batchmean'
    )
    hard_loss = F.cross_entropy(student_logits, labels)
    return 0.7 * soft_loss + 0.3 * hard_loss
```

---

## ğŸ“š å­¦ä¹ èµ„æº

### 1. ç†è®ºåŸºç¡€

- **ResNet**: æ®‹å·®ç½‘ç»œåŸç†
- **Focal Loss**: éš¾æ ·æœ¬æŒ–æ˜
- **æ··åˆç²¾åº¦**: AMPè®­ç»ƒæŠ€æœ¯
- **é›†æˆå­¦ä¹ **: Stackingæ–¹æ³•

### 2. å®è·µæŠ€å·§

- **è¶…å‚æ•°è°ƒä¼˜**: ç½‘æ ¼æœç´¢ã€è´å¶æ–¯ä¼˜åŒ–
- **æ¨¡å‹è°ƒè¯•**: æ¢¯åº¦æ£€æŸ¥ã€æ¿€æ´»å¯è§†åŒ–
- **æ€§èƒ½åˆ†æ**: æ—¶é—´åˆ†æã€å†…å­˜åˆ†æ

### 3. å·¥å…·æ¨è

- **ç›‘æ§**: TensorBoard, Weights & Biases
- **ä¼˜åŒ–**: Optuna, Ray Tune
- **è°ƒè¯•**: PyTorch Profiler, torchviz

---

**å¼€å§‹ä½ çš„è®­ç»ƒä¹‹æ—…å§ï¼** ğŸš€

é€šè¿‡è¿™ä¸ªå®Œæ•´çš„è®­ç»ƒæŒ‡å—ï¼Œä½ å¯ä»¥ï¼š
- ç†è§£æ¯ä¸ªé˜¶æ®µçš„æŠ€æœ¯åˆ›æ–°
- æŒæ¡ç°ä»£æ·±åº¦å­¦ä¹ è®­ç»ƒæŠ€å·§
- å¤ç°ä»57%åˆ°93%çš„ä¼˜åŒ–å†ç¨‹
- ä¸ºä½ çš„é¡¹ç›®æä¾›å‚è€ƒ
