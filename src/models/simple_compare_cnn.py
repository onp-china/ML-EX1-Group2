"""
MNISTæ•°å­—æ¯”è¾ƒæ¨¡å‹å®šä¹‰æ–‡ä»¶
============================

æœ¬æ–‡ä»¶åŒ…å«ç”¨äºæ¯”è¾ƒä¸¤ä¸ªMNISTæ•°å­—æ˜¯å¦ç›¸åŒçš„ç¥ç»ç½‘ç»œæ¨¡å‹ï¼š
1. CompareNet - åŸºçº¿æ¨¡å‹ï¼šç®€å•çš„CNN + ç‰¹å¾æ‹¼æ¥
2. ImprovedCompareNet - æ”¹è¿›æ¨¡å‹ï¼šæ·±åº¦CNN + æ³¨æ„åŠ›æœºåˆ¶ + å¤šç§èåˆæ–¹å¼
3. ResNetCompareNet - ResNetæ¨¡å‹ï¼šResNetæ¶æ„ + æ³¨æ„åŠ›æœºåˆ¶ + å¤šç§èåˆæ–¹å¼

ä»»åŠ¡ï¼šäºŒåˆ†ç±»é—®é¢˜ï¼Œåˆ¤æ–­ä¸¤ä¸ª28x28çš„MNISTæ•°å­—å›¾åƒæ˜¯å¦ç›¸åŒ
è¾“å…¥ï¼šä¸¤ä¸ª28x28çš„å•é€šé“å›¾åƒ
è¾“å‡ºï¼šäºŒåˆ†ç±»logitï¼ˆç›¸åŒ=1ï¼Œä¸åŒ=0ï¼‰
"""

import torch
import torch.nn as nn
import torch.nn.functional as F

class BasicBlock(nn.Module):
    """
    ResNetåŸºæœ¬å—
    ===========
    
    åŠŸèƒ½ï¼šResNetçš„åŸºæœ¬æ„å»ºå•å…ƒï¼ŒåŒ…å«æ®‹å·®è¿æ¥
    ç‰¹ç‚¹ï¼šè§£å†³æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ï¼Œæ”¯æŒæ›´æ·±çš„ç½‘ç»œ
    
    ç»“æ„ï¼š
    input -> conv1 -> bn1 -> relu -> conv2 -> bn2 -> + -> relu -> output
           |                                        |
           |---------> shortcut (if needed) ------->|
    """
    expansion = 1
    
    def __init__(self, in_planes, planes, stride=1, downsample=None):
        super(BasicBlock, self).__init__()
        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(planes)
        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(planes)
        self.downsample = downsample
        
    def forward(self, x):
        residual = x
        
        out = F.relu(self.bn1(self.conv1(x)))
        out = self.bn2(self.conv2(out))
        
        if self.downsample is not None:
            residual = self.downsample(x)
            
        out += residual
        out = F.relu(out)
        return out

class Bottleneck(nn.Module):
    """
    ResNetç“¶é¢ˆå—
    ===========
    
    åŠŸèƒ½ï¼šResNetçš„ç“¶é¢ˆç»“æ„ï¼Œå‡å°‘å‚æ•°é‡
    ç‰¹ç‚¹ï¼š1x1 -> 3x3 -> 1x1 çš„å·ç§¯åºåˆ—
    
    ç»“æ„ï¼š
    input -> 1x1conv -> bn1 -> relu -> 3x3conv -> bn2 -> relu -> 1x1conv -> bn3 -> + -> relu -> output
           |                                                                      |
           |-------------------------> shortcut (if needed) --------------------->|
    """
    expansion = 4
    
    def __init__(self, in_planes, planes, stride=1, downsample=None):
        super(Bottleneck, self).__init__()
        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)
        self.bn1 = nn.BatchNorm2d(planes)
        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(planes)
        self.conv3 = nn.Conv2d(planes, planes * self.expansion, kernel_size=1, bias=False)
        self.bn3 = nn.BatchNorm2d(planes * self.expansion)
        self.downsample = downsample
        
    def forward(self, x):
        residual = x
        
        out = F.relu(self.bn1(self.conv1(x)))
        out = F.relu(self.bn2(self.conv2(out)))
        out = self.bn3(self.conv3(out))
        
        if self.downsample is not None:
            residual = self.downsample(x)
            
        out += residual
        out = F.relu(out)
        return out

class AttentionModule(nn.Module):
    """
    æ”¹è¿›çš„æ³¨æ„åŠ›æœºåˆ¶æ¨¡å—
    ==================
    
    åŠŸèƒ½ï¼šä¸ºç‰¹å¾å‘é‡å­¦ä¹ æ³¨æ„åŠ›æƒé‡ï¼Œçªå‡ºé‡è¦ç‰¹å¾
    ç‰¹ç‚¹ï¼šä½¿ç”¨æ®‹å·®è¿æ¥å’ŒLayerNormï¼Œæé«˜è®­ç»ƒç¨³å®šæ€§
    
    è¾“å…¥ï¼šfeat_dimç»´çš„ç‰¹å¾å‘é‡
    è¾“å‡ºï¼šåŠ æƒåçš„feat_dimç»´ç‰¹å¾å‘é‡
    
    æ”¹è¿›ç‚¹ï¼š
    1. æ·»åŠ æ®‹å·®è¿æ¥ï¼šè§£å†³æ¢¯åº¦æ¶ˆå¤±é—®é¢˜
    2. ä½¿ç”¨LayerNormï¼šæé«˜è®­ç»ƒç¨³å®šæ€§
    3. ä½¿ç”¨GELUæ¿€æ´»ï¼šæ›´å¥½çš„éçº¿æ€§è¡¨è¾¾èƒ½åŠ›
    """
    def __init__(self, feat_dim):
        super().__init__()
        # LayerNormå±‚
        self.norm1 = nn.LayerNorm(feat_dim)
        self.norm2 = nn.LayerNorm(feat_dim)
        
        # æ³¨æ„åŠ›ç½‘ç»œï¼šé™ç»´ -> æ¿€æ´» -> å‡ç»´ -> å½’ä¸€åŒ–
        self.attention = nn.Sequential(
            nn.Linear(feat_dim, feat_dim // 4),  # é™ç»´åˆ°1/4
            nn.GELU(),                           # æ›´å¥½çš„æ¿€æ´»å‡½æ•°
            nn.Linear(feat_dim // 4, feat_dim),  # å‡ç»´å›åŸç»´åº¦
            nn.Sigmoid()                         # è¾“å‡º0-1çš„æ³¨æ„åŠ›æƒé‡
        )
    
    def forward(self, x):
        # æ®‹å·®è¿æ¥ + LayerNorm
        residual = x
        x = self.norm1(x)
        attn_weights = self.attention(x)
        x = x * attn_weights
        return self.norm2(x + residual)

class ResNetTower(nn.Module):
    """
    ResNetç‰¹å¾æå–å¡”
    ===============
    
    åŠŸèƒ½ï¼šåŸºäºResNetæ¶æ„çš„ç‰¹å¾æå–å™¨ï¼Œæ”¯æŒæ›´æ·±çš„ç½‘ç»œ
    ç‰¹ç‚¹ï¼šä½¿ç”¨æ®‹å·®è¿æ¥ï¼Œè§£å†³æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ï¼Œæ”¯æŒå¤šå±‚ç»“æ„
    
    ç½‘ç»œç»“æ„ï¼š
    28x28x1 -> 14x14x64 -> 7x7x128 -> 3x3x256 -> 1x1x512 -> out_dimç»´ç‰¹å¾
    
    å‚æ•°é‡ï¼šçº¦200ä¸‡å‚æ•°ï¼ˆç›¸æ¯”ImprovedTowerçš„72ä¸‡å‚æ•°ï¼‰
    
    æ”¹è¿›ç‚¹ï¼š
    1. ä½¿ç”¨ResNetåŸºæœ¬å—ï¼Œæ”¯æŒæ®‹å·®è¿æ¥
    2. æ›´æ·±çš„ç½‘ç»œç»“æ„ï¼ˆå¤šå±‚ResNetå—ï¼‰
    3. æ›´å¥½çš„ç‰¹å¾æå–èƒ½åŠ›
    4. è§£å†³æ¢¯åº¦æ¶ˆå¤±é—®é¢˜
    """
    def __init__(self, out_dim=256, layers=[2, 2, 2, 2], block=BasicBlock):
        super(ResNetTower, self).__init__()
        self.in_planes = 64
        
        # åˆå§‹å·ç§¯å±‚ - ä¿®å¤è¿‡åº¦ä¸‹é‡‡æ ·é—®é¢˜
        self.conv1 = nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1, bias=False)  # 28x28x1 -> 28x28x64
        self.bn1 = nn.BatchNorm2d(64)
        # ç§»é™¤maxpoolï¼Œé¿å…è¿‡æ—©ä¸¢å¤±ç©ºé—´ä¿¡æ¯
        # self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)
        
        # ResNetå±‚ - è°ƒæ•´strideä»¥é€‚åº”æ–°çš„è¾“å…¥å°ºå¯¸
        self.layer1 = self._make_layer(block, 64, layers[0], stride=1)   # 28x28x64 -> 28x28x64
        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)  # 28x28x64 -> 14x14x128
        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)  # 14x14x128 -> 7x7x256
        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)  # 7x7x256 -> 4x4x512
        
        # å…¨å±€å¹³å‡æ± åŒ–å’Œåˆ†ç±»å™¨
        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))  # 1x1x512 -> 1x1x512
        # è®¡ç®—æœ€ç»ˆç‰¹å¾ç»´åº¦ï¼ˆè€ƒè™‘blockçš„expansionï¼‰
        final_dim = 512 * block.expansion
        self.fc = nn.Sequential(
            nn.Flatten(),                        # 1x1x512*expansion -> 512*expansion
            nn.Linear(final_dim, out_dim),       # 512*expansion -> out_dim
            nn.ReLU(inplace=True),
            nn.Dropout(0.3),                     # é˜²æ­¢è¿‡æ‹Ÿåˆ
        )
        
    def _make_layer(self, block, planes, blocks, stride=1):
        """æ„å»ºResNetå±‚"""
        downsample = None
        if stride != 1 or self.in_planes != planes * block.expansion:
            downsample = nn.Sequential(
                nn.Conv2d(self.in_planes, planes * block.expansion,
                         kernel_size=1, stride=stride, bias=False),
                nn.BatchNorm2d(planes * block.expansion),
            )
        
        layers = []
        layers.append(block(self.in_planes, planes, stride, downsample))
        self.in_planes = planes * block.expansion
        for _ in range(1, blocks):
            layers.append(block(self.in_planes, planes))
        
        return nn.Sequential(*layers)

class SEModule(nn.Module):
    """
    SEæ³¨æ„åŠ›æ¨¡å—
    ===========
    
    åŠŸèƒ½ï¼šSqueeze-and-Excitationæ³¨æ„åŠ›æœºåˆ¶ï¼Œå­¦ä¹ é€šé“é—´çš„ä¾èµ–å…³ç³»
    ç‰¹ç‚¹ï¼šé€šè¿‡å…¨å±€å¹³å‡æ± åŒ–å’Œå…¨è¿æ¥å±‚å­¦ä¹ é€šé“æƒé‡
    
    å‚æ•°ï¼š
    - channels: è¾“å…¥é€šé“æ•°
    - reduction: é™ç»´æ¯”ä¾‹ï¼Œé»˜è®¤16
    """
    def __init__(self, channels, reduction=16):
        super(SEModule, self).__init__()
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        self.fc = nn.Sequential(
            nn.Linear(channels, channels // reduction, bias=False),
            nn.ReLU(inplace=True),
            nn.Linear(channels // reduction, channels, bias=False),
            nn.Sigmoid()
        )
    
    def forward(self, x):
        b, c, _, _ = x.size()
        y = self.avg_pool(x).view(b, c)
        y = self.fc(y).view(b, c, 1, 1)
        return x * y.expand_as(x)

class ResNetTower(nn.Module):
    """
    ResNetç‰¹å¾æå–å¡”ï¼ˆ3å±‚ç‰ˆæœ¬ï¼‰
    =========================
    
    åŠŸèƒ½ï¼šåŸºäºResNetæ¶æ„çš„ç‰¹å¾æå–å™¨ï¼Œé’ˆå¯¹MNISTä¼˜åŒ–
    ç‰¹ç‚¹ï¼šä½¿ç”¨æ®‹å·®è¿æ¥ï¼Œ3å±‚ç»“æ„ï¼Œé¿å…è¿‡åº¦å¤æ‚
    
    ç½‘ç»œç»“æ„ï¼š
    28x28x1 -> 28x28x64 -> 14x14x128 -> 7x7x256 -> out_dimç»´ç‰¹å¾
    
    å‚æ•°é‡ï¼šçº¦100ä¸‡å‚æ•°ï¼ˆç›¸æ¯”4å±‚ç‰ˆæœ¬å‡å°‘50%ï¼‰
    
    æ”¹è¿›ç‚¹ï¼š
    1. ä½¿ç”¨ResNetåŸºæœ¬å—ï¼Œæ”¯æŒæ®‹å·®è¿æ¥
    2. 3å±‚ç»“æ„ï¼Œé€‚åˆMNISTå°å›¾åƒ
    3. é¿å…è¿‡åº¦å¤æ‚ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ
    4. è§£å†³æ¢¯åº¦æ¶ˆå¤±é—®é¢˜
    """
    def __init__(self, out_dim=256, layers=[2, 2, 2], block=BasicBlock, width_mult: float = 1.0):
        super(ResNetTower, self).__init__()
        self.width_mult = float(width_mult)
        c64 = int(64 * self.width_mult)
        c128 = int(128 * self.width_mult)
        c256 = int(256 * self.width_mult)
        self.in_planes = c64
        
        # åˆå§‹å·ç§¯å±‚ - ä¿®å¤è¿‡åº¦ä¸‹é‡‡æ ·é—®é¢˜
        self.conv1 = nn.Conv2d(1, c64, kernel_size=3, stride=1, padding=1, bias=False)  # 28x28x1 -> 28x28xC64
        self.bn1 = nn.BatchNorm2d(c64)
        # ç§»é™¤maxpoolï¼Œé¿å…è¿‡æ—©ä¸¢å¤±ç©ºé—´ä¿¡æ¯
        # self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)
        
        # ResNetå±‚ - 3å±‚ç»“æ„ï¼Œé€‚åˆMNIST
        self.layer1 = self._make_layer(block, c64, layers[0], stride=1)   # 28x28xC64 -> 28x28xC64
        self.layer2 = self._make_layer(block, c128, layers[1], stride=2)  # 28x28xC64 -> 14x14xC128
        self.layer3 = self._make_layer(block, c256, layers[2], stride=2)  # 14x14xC128 -> 7x7xC256
        
        # å…¨å±€å¹³å‡æ± åŒ–å’Œåˆ†ç±»å™¨
        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))  # 7x7x256 -> 1x1x256
        # è®¡ç®—æœ€ç»ˆç‰¹å¾ç»´åº¦ï¼ˆè€ƒè™‘blockçš„expansionï¼‰
        final_dim = c256 * block.expansion
        self.fc = nn.Sequential(
            nn.Flatten(),                        # 1x1x256*expansion -> 256*expansion
            nn.Linear(final_dim, out_dim),       # 256*expansion -> out_dim
            nn.BatchNorm1d(out_dim),             # æ‰¹å½’ä¸€åŒ–
            nn.ReLU(inplace=True),
            nn.Dropout(0.4),                     # å¢åŠ dropoutç‡
            nn.Linear(out_dim, out_dim),         # é¢å¤–å…¨è¿æ¥å±‚
            nn.BatchNorm1d(out_dim),             # æ‰¹å½’ä¸€åŒ–
            nn.ReLU(inplace=True),
            nn.Dropout(0.3),                     # é˜²æ­¢è¿‡æ‹Ÿåˆ
        )
        
        # æ·»åŠ SEæ¨¡å—ï¼ˆ3å±‚ç‰ˆæœ¬ï¼‰
        self.se1 = SEModule(c64)
        self.se2 = SEModule(c128)
        self.se3 = SEModule(c256)
        
    def _make_layer(self, block, planes, blocks, stride=1):
        """æ„å»ºResNetå±‚"""
        downsample = None
        if stride != 1 or self.in_planes != planes * block.expansion:
            downsample = nn.Sequential(
                nn.Conv2d(self.in_planes, planes * block.expansion,
                         kernel_size=1, stride=stride, bias=False),
                nn.BatchNorm2d(planes * block.expansion),
            )
        
        layers = []
        layers.append(block(self.in_planes, planes, stride, downsample))
        self.in_planes = planes * block.expansion
        for _ in range(1, blocks):
            layers.append(block(self.in_planes, planes))
        
        return nn.Sequential(*layers)
    
    def forward(self, x):
        # åˆå§‹å·ç§¯ - ä¿æŒç©ºé—´åˆ†è¾¨ç‡
        x = F.relu(self.bn1(self.conv1(x)))  # 28x28x1 -> 28x28x64
        # ç§»é™¤maxpoolï¼Œé¿å…è¿‡æ—©ä¸¢å¤±ç©ºé—´ä¿¡æ¯
        
        # ResNetå±‚ + SEæ³¨æ„åŠ›ï¼ˆ3å±‚ç‰ˆæœ¬ï¼‰
        x = self.layer1(x)  # 28x28x64 -> 28x28x64
        x = self.se1(x)     # åº”ç”¨SEæ³¨æ„åŠ›
        
        x = self.layer2(x)  # 28x28x64 -> 14x14x128
        x = self.se2(x)     # åº”ç”¨SEæ³¨æ„åŠ›
        
        x = self.layer3(x)  # 14x14x128 -> 7x7x256
        x = self.se3(x)     # åº”ç”¨SEæ³¨æ„åŠ›
        
        # å…¨å±€å¹³å‡æ± åŒ–å’Œåˆ†ç±»
        x = self.avgpool(x)  # 7x7x256 -> 1x1x256
        x = self.fc(x)       # 1x1x256 -> out_dim
        
        return x

class ImprovedTower(nn.Module):
    """
    æ”¹è¿›çš„ç‰¹å¾æå–å¡”
    ===============
    
    åŠŸèƒ½ï¼šä»28x28çš„MNISTå›¾åƒä¸­æå–256ç»´ç‰¹å¾å‘é‡
    ç‰¹ç‚¹ï¼šæ›´æ·±çš„ç½‘ç»œç»“æ„ï¼Œæ›´å¼ºçš„ç‰¹å¾æå–èƒ½åŠ›
    
    ç½‘ç»œç»“æ„ï¼š
    28x28x1 -> 14x14x64 -> 7x7x128 -> 1x1x256 -> 256ç»´ç‰¹å¾
    
    å‚æ•°é‡ï¼šçº¦72ä¸‡å‚æ•°ï¼ˆç›¸æ¯”åŸºçº¿æ¨¡å‹çš„8ä¸‡å‚æ•°ï¼‰
    
    ğŸš€ æ”¹è¿›å»ºè®®ï¼š
    1. ä½¿ç”¨ResNetå—ï¼šæ·»åŠ æ®‹å·®è¿æ¥ï¼Œè§£å†³æ¢¯åº¦æ¶ˆå¤±é—®é¢˜
    2. ä½¿ç”¨SEæ¨¡å—ï¼šSqueeze-and-Excitationæ³¨æ„åŠ›æœºåˆ¶
    3. ä½¿ç”¨æ›´å…ˆè¿›çš„backboneï¼šEfficientNetã€ResNeXtç­‰
    4. æ·»åŠ æ›´å¤šæ­£åˆ™åŒ–ï¼šDropBlockã€Stochastic Depthç­‰
    5. ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒï¼šæé«˜è®­ç»ƒæ•ˆç‡
    """
    def __init__(self, out_dim=256):
        super().__init__()
        self.conv = nn.Sequential(
            # ç¬¬ä¸€ç»„ï¼šæå–ä½çº§ç‰¹å¾ï¼ˆè¾¹ç¼˜ã€çº¹ç†ï¼‰
            nn.Conv2d(1, 64, 3, padding=1),      # 28x28x1 -> 28x28x64
            nn.BatchNorm2d(64), nn.ReLU(inplace=True),
            nn.Conv2d(64, 64, 3, padding=1),     # 28x28x64 -> 28x28x64
            nn.BatchNorm2d(64), nn.ReLU(inplace=True),
            nn.MaxPool2d(2),                     # 28x28x64 -> 14x14x64
            
            # ç¬¬äºŒç»„ï¼šæå–ä¸­çº§ç‰¹å¾ï¼ˆå½¢çŠ¶ã€ç»“æ„ï¼‰
            nn.Conv2d(64, 128, 3, padding=1),    # 14x14x64 -> 14x14x128
            nn.BatchNorm2d(128), nn.ReLU(inplace=True),
            nn.Conv2d(128, 128, 3, padding=1),   # 14x14x128 -> 14x14x128
            nn.BatchNorm2d(128), nn.ReLU(inplace=True),
            nn.MaxPool2d(2),                     # 14x14x128 -> 7x7x128
            
            # ç¬¬ä¸‰ç»„ï¼šæå–é«˜çº§ç‰¹å¾ï¼ˆè¯­ä¹‰ä¿¡æ¯ï¼‰
            nn.Conv2d(128, 256, 3, padding=1),   # 7x7x128 -> 7x7x256
            nn.BatchNorm2d(256), nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, 3, padding=1),   # 7x7x256 -> 7x7x256
            nn.BatchNorm2d(256), nn.ReLU(inplace=True),
            nn.AdaptiveAvgPool2d((1,1)),         # 7x7x256 -> 1x1x256
        )
        self.fc = nn.Sequential(
            nn.Flatten(),                        # 1x1x256 -> 256
            nn.Linear(256, out_dim),             # 256 -> out_dim
            nn.ReLU(inplace=True),
            nn.Dropout(0.3),                     # é˜²æ­¢è¿‡æ‹Ÿåˆ
        )
        
        # ğŸš€ æ”¹è¿›å»ºè®®ï¼šæ·»åŠ SEæ¨¡å—
        # self.se = SEModule(out_dim)
        
        # ğŸš€ æ”¹è¿›å»ºè®®ï¼šæ·»åŠ ResNetå—
        # self.res_blocks = nn.ModuleList([
        #     ResNetBlock(64, 64),
        #     ResNetBlock(128, 128),
        #     ResNetBlock(256, 256)
        # ])

    def forward(self, x):
        x = self.conv(x)  # å·ç§¯ç‰¹å¾æå–
        x = self.fc(x)    # å…¨è¿æ¥é™ç»´
        
        # ğŸš€ æ”¹è¿›å»ºè®®ï¼šæ·»åŠ SEæ³¨æ„åŠ›
        # x = self.se(x)
        
        return x

class Tower(nn.Module):
    """
    åŸºçº¿ç‰¹å¾æå–å¡”
    =============
    
    åŠŸèƒ½ï¼šä»28x28çš„MNISTå›¾åƒä¸­æå–128ç»´ç‰¹å¾å‘é‡
    ç‰¹ç‚¹ï¼šç®€å•çš„3å±‚å·ç§¯ç»“æ„ï¼Œå‚æ•°é‡å°‘ï¼Œè®­ç»ƒå¿«é€Ÿ
    
    ç½‘ç»œç»“æ„ï¼š
    28x28x1 -> 14x14x32 -> 7x7x64 -> 1x1x128 -> 128ç»´ç‰¹å¾
    
    å‚æ•°é‡ï¼šçº¦8ä¸‡å‚æ•°
    """
    def __init__(self, out_dim=128):
        super().__init__()
        self.conv = nn.Sequential(
            # ç¬¬ä¸€å±‚ï¼šåŸºç¡€ç‰¹å¾æå–
            nn.Conv2d(1, 32, 3, padding=1),      # 28x28x1 -> 28x28x32
            nn.BatchNorm2d(32), nn.ReLU(inplace=True),
            nn.MaxPool2d(2),                     # 28x28x32 -> 14x14x32
            
            # ç¬¬äºŒå±‚ï¼šä¸­çº§ç‰¹å¾æå–
            nn.Conv2d(32, 64, 3, padding=1),     # 14x14x32 -> 14x14x64
            nn.BatchNorm2d(64), nn.ReLU(inplace=True),
            nn.MaxPool2d(2),                     # 14x14x64 -> 7x7x64
            
            # ç¬¬ä¸‰å±‚ï¼šé«˜çº§ç‰¹å¾æå–
            nn.Conv2d(64, 128, 3, padding=1),    # 7x7x64 -> 7x7x128
            nn.BatchNorm2d(128), nn.ReLU(inplace=True),
            nn.AdaptiveAvgPool2d((1,1)),         # 7x7x128 -> 1x1x128
        )
        self.fc = nn.Sequential(
            nn.Flatten(),                        # 1x1x128 -> 128
            nn.Linear(128, out_dim),             # 128 -> out_dim
            nn.ReLU(inplace=True),
            nn.Dropout(0.1),                     # è½»å¾®æ­£åˆ™åŒ–
        )

    def forward(self, x):
        x = self.conv(x)  # å·ç§¯ç‰¹å¾æå–
        x = self.fc(x)    # å…¨è¿æ¥é™ç»´
        return x

class ResNetCompareNet(nn.Module):
    """
    ResNetæ•°å­—æ¯”è¾ƒç½‘ç»œ
    =================
    
    åŠŸèƒ½ï¼šåŸºäºResNetæ¶æ„çš„æ•°å­—æ¯”è¾ƒç½‘ç»œï¼Œåˆ¤æ–­ä¸¤ä¸ªMNISTæ•°å­—æ˜¯å¦ç›¸åŒ
    ç‰¹ç‚¹ï¼šä½¿ç”¨ResNetç‰¹å¾æå– + æ³¨æ„åŠ›æœºåˆ¶ + å¤šç§ç‰¹å¾èåˆæ–¹å¼
    
    åˆ›æ–°ç‚¹ï¼š
    1. ResNetç‰¹å¾æå–ï¼šæ›´æ·±çš„ç½‘ç»œï¼Œæ®‹å·®è¿æ¥
    2. æ³¨æ„åŠ›æœºåˆ¶ï¼šçªå‡ºé‡è¦ç‰¹å¾
    3. å¤šç§èåˆï¼šå·®å€¼ã€æ‹¼æ¥ã€ä¹˜ç§¯ä¸‰ç§æ–¹å¼
    4. è‡ªé€‚åº”æƒé‡ï¼šå­¦ä¹ æœ€ä¼˜èåˆæ¯”ä¾‹
    
    é¢„æœŸæ€§èƒ½ï¼šå‡†ç¡®ç‡85-90%ï¼ˆç›¸æ¯”ImprovedCompareNetçš„80.59%ï¼‰
    å‚æ•°é‡ï¼šçº¦300ä¸‡å‚æ•°ï¼ˆç›¸æ¯”ImprovedCompareNetçš„144ä¸‡ï¼‰
    """
    def __init__(self, feat_dim=256, layers=[2, 2, 2], block=BasicBlock, width_mult=1.0):
        super().__init__()
        # ResNetç‰¹å¾æå–å¡”
        self.tower = ResNetTower(out_dim=feat_dim, layers=layers, block=block, width_mult=width_mult)
        # æ³¨æ„åŠ›æœºåˆ¶
        self.attention = AttentionModule(feat_dim)
        
        # äº”ç§ä¸åŒçš„ç‰¹å¾èåˆæ–¹å¼ï¼ˆå¢å¼ºæ­£åˆ™åŒ–ï¼‰
        # 1. å·®å€¼èåˆï¼š|fa - fb|ï¼Œå…³æ³¨å·®å¼‚
        self.diff_head = nn.Sequential(
            nn.Linear(feat_dim, 128),
            nn.BatchNorm1d(128),
            nn.ReLU(),
            nn.Dropout(0.4),
            nn.Linear(128, 64),
            nn.BatchNorm1d(64),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(64, 1)
        )
        
        # 2. æ‹¼æ¥èåˆï¼š[fa, fb]ï¼Œä¿ç•™å®Œæ•´ä¿¡æ¯
        self.concat_head = nn.Sequential(
            nn.Linear(feat_dim * 2, 256),
            nn.BatchNorm1d(256),
            nn.ReLU(),
            nn.Dropout(0.4),
            nn.Linear(256, 128),
            nn.BatchNorm1d(128),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(128, 1)
        )
        
        # 3. ä¹˜ç§¯èåˆï¼šfa * fbï¼Œå…³æ³¨ç›¸ä¼¼æ€§
        self.product_head = nn.Sequential(
            nn.Linear(feat_dim, 128),
            nn.BatchNorm1d(128),
            nn.ReLU(),
            nn.Dropout(0.4),
            nn.Linear(128, 64),
            nn.BatchNorm1d(64),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(64, 1)
        )
        
        # 4. ä½™å¼¦ç›¸ä¼¼åº¦èåˆï¼šcos(fa, fb)ï¼Œå…³æ³¨æ–¹å‘ç›¸ä¼¼æ€§
        self.cosine_head = nn.Sequential(
            nn.Linear(1, 64),  # ä½™å¼¦ç›¸ä¼¼åº¦æ˜¯æ ‡é‡
            nn.BatchNorm1d(64),
            nn.ReLU(),
            nn.Dropout(0.4),
            nn.Linear(64, 32),
            nn.BatchNorm1d(32),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(32, 1)
        )
        
        # 5. æ¬§æ°è·ç¦»èåˆï¼š||fa - fb||ï¼Œå…³æ³¨è·ç¦»ç›¸ä¼¼æ€§
        self.distance_head = nn.Sequential(
            nn.Linear(1, 64),  # è·ç¦»æ˜¯æ ‡é‡
            nn.BatchNorm1d(64),
            nn.ReLU(),
            nn.Dropout(0.4),
            nn.Linear(64, 32),
            nn.BatchNorm1d(32),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(32, 1)
        )
        
        # å¯å­¦ä¹ çš„èåˆæƒé‡ï¼ˆåˆå§‹åŒ–ä¸ºå‡åŒ€åˆ†å¸ƒï¼Œç°åœ¨æœ‰5ä¸ªèåˆæ–¹å¼ï¼‰
        self.fusion_weights = nn.Parameter(torch.ones(5) / 5)
    
    def forward(self, xa, xb):
        # 1. ResNetç‰¹å¾æå–
        fa = self.tower(xa)  # æå–å›¾åƒAçš„ç‰¹å¾
        fb = self.tower(xb)  # æå–å›¾åƒBçš„ç‰¹å¾
        
        # 2. æ³¨æ„åŠ›åŠ æƒ
        fa = self.attention(fa)  # å¯¹ç‰¹å¾Aåº”ç”¨æ³¨æ„åŠ›
        fb = self.attention(fb)  # å¯¹ç‰¹å¾Båº”ç”¨æ³¨æ„åŠ›
        
        # 3. å¤šç§ç‰¹å¾èåˆæ–¹å¼
        diff = torch.abs(fa - fb)                    # å·®å€¼ï¼šå…³æ³¨å·®å¼‚
        concat = torch.cat([fa, fb], dim=-1)         # æ‹¼æ¥ï¼šä¿ç•™å®Œæ•´ä¿¡æ¯
        product = fa * fb                            # ä¹˜ç§¯ï¼šå…³æ³¨ç›¸ä¼¼æ€§
        
        # 4. åˆ†åˆ«é€šè¿‡ä¸åŒçš„åˆ†ç±»å¤´
        diff_logit = self.diff_head(diff)            # å·®å€¼åˆ†æ”¯çš„logit
        concat_logit = self.concat_head(concat)      # æ‹¼æ¥åˆ†æ”¯çš„logit
        product_logit = self.product_head(product)   # ä¹˜ç§¯åˆ†æ”¯çš„logit
        
        # 5. è‡ªé€‚åº”æƒé‡èåˆ
        weights = F.softmax(self.fusion_weights, dim=0)  # å½’ä¸€åŒ–æƒé‡
        logit = weights[0] * diff_logit + weights[1] * concat_logit + weights[2] * product_logit
        
        return logit.squeeze(1)  # ç§»é™¤å¤šä½™çš„ç»´åº¦

class ImprovedCompareNet(nn.Module):
    """
    æ”¹è¿›çš„æ•°å­—æ¯”è¾ƒç½‘ç»œ
    =================
    
    åŠŸèƒ½ï¼šåˆ¤æ–­ä¸¤ä¸ªMNISTæ•°å­—æ˜¯å¦ç›¸åŒï¼ˆäºŒåˆ†ç±»ï¼‰
    ç‰¹ç‚¹ï¼šä½¿ç”¨æ³¨æ„åŠ›æœºåˆ¶ + å¤šç§ç‰¹å¾èåˆæ–¹å¼ + è‡ªé€‚åº”æƒé‡èåˆ
    
    åˆ›æ–°ç‚¹ï¼š
    1. æ³¨æ„åŠ›æœºåˆ¶ï¼šçªå‡ºé‡è¦ç‰¹å¾
    2. å¤šç§èåˆï¼šå·®å€¼ã€æ‹¼æ¥ã€ä¹˜ç§¯ä¸‰ç§æ–¹å¼
    3. è‡ªé€‚åº”æƒé‡ï¼šå­¦ä¹ æœ€ä¼˜èåˆæ¯”ä¾‹
    
    æ€§èƒ½ï¼šå‡†ç¡®ç‡80.59%ï¼ŒF1åˆ†æ•°80.59%ï¼ˆç›¸æ¯”åŸºçº¿æ¨¡å‹57.11%ï¼‰
    å‚æ•°é‡ï¼š144ä¸‡å‚æ•°ï¼ˆç›¸æ¯”åŸºçº¿æ¨¡å‹17.5ä¸‡ï¼‰
    
    ğŸš€ æ”¹è¿›å»ºè®®ï¼š
    1. æ·»åŠ æ›´å¤šèåˆæ–¹å¼ï¼šä½™å¼¦ç›¸ä¼¼åº¦ã€æ¬§å‡ é‡Œå¾—è·ç¦»ç­‰
    2. ä½¿ç”¨Transformeræ¶æ„ï¼šè‡ªæ³¨æ„åŠ›æœºåˆ¶
    3. æ·»åŠ å¯¹æ¯”å­¦ä¹ ï¼šå­¦ä¹ æ›´å¥½çš„ç‰¹å¾è¡¨ç¤º
    4. ä½¿ç”¨æ¨¡å‹é›†æˆï¼šå¤šä¸ªæ¨¡å‹çš„é¢„æµ‹ç»“æœèåˆ
    5. æ·»åŠ è¾…åŠ©ä»»åŠ¡ï¼šæ•°å­—åˆ†ç±»ã€æ—‹è½¬é¢„æµ‹ç­‰
    """
    def __init__(self, feat_dim=256):
        super().__init__()
        # ç‰¹å¾æå–å¡”
        self.tower = ImprovedTower(out_dim=feat_dim)
        # æ³¨æ„åŠ›æœºåˆ¶
        self.attention = AttentionModule(feat_dim)
        
        # ä¸‰ç§ä¸åŒçš„ç‰¹å¾èåˆæ–¹å¼
        # 1. å·®å€¼èåˆï¼š|fa - fb|ï¼Œå…³æ³¨å·®å¼‚
        self.diff_head = nn.Sequential(
            nn.Linear(feat_dim, 128),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(128, 1)
        )
        
        # 2. æ‹¼æ¥èåˆï¼š[fa, fb]ï¼Œä¿ç•™å®Œæ•´ä¿¡æ¯
        self.concat_head = nn.Sequential(
            nn.Linear(feat_dim * 2, 256),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(256, 1)
        )
        
        # 3. ä¹˜ç§¯èåˆï¼šfa * fbï¼Œå…³æ³¨ç›¸ä¼¼æ€§
        self.product_head = nn.Sequential(
            nn.Linear(feat_dim, 128),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(128, 1)
        )
        
        # ğŸš€ æ”¹è¿›å»ºè®®ï¼šæ·»åŠ æ›´å¤šèåˆæ–¹å¼
        # 4. ä½™å¼¦ç›¸ä¼¼åº¦èåˆ
        # self.cosine_head = nn.Sequential(
        #     nn.Linear(1, 64),  # ä½™å¼¦ç›¸ä¼¼åº¦æ˜¯æ ‡é‡
        #     nn.ReLU(),
        #     nn.Linear(64, 1)
        # )
        
        # 5. æ¬§å‡ é‡Œå¾—è·ç¦»èåˆ
        # self.distance_head = nn.Sequential(
        #     nn.Linear(1, 64),  # è·ç¦»æ˜¯æ ‡é‡
        #     nn.ReLU(),
        #     nn.Linear(64, 1)
        # )
        
        # å¯å­¦ä¹ çš„èåˆæƒé‡ï¼ˆåˆå§‹åŒ–ä¸ºå‡åŒ€åˆ†å¸ƒï¼‰
        self.fusion_weights = nn.Parameter(torch.ones(3) / 3)
        
        # ğŸš€ æ”¹è¿›å»ºè®®ï¼šåŠ¨æ€æƒé‡å­¦ä¹ 
        # self.weight_net = nn.Sequential(
        #     nn.Linear(feat_dim * 2, 128),
        #     nn.ReLU(),
        #     nn.Linear(128, 3),
        #     nn.Softmax(dim=-1)
        # )
    
    def forward(self, xa, xb):
        # 1. ç‰¹å¾æå–
        fa = self.tower(xa)  # æå–å›¾åƒAçš„ç‰¹å¾
        fb = self.tower(xb)  # æå–å›¾åƒBçš„ç‰¹å¾
        
        # 2. æ³¨æ„åŠ›åŠ æƒ
        fa = self.attention(fa)  # å¯¹ç‰¹å¾Aåº”ç”¨æ³¨æ„åŠ›
        fb = self.attention(fb)  # å¯¹ç‰¹å¾Båº”ç”¨æ³¨æ„åŠ›
        
        # 3. å¤šç§ç‰¹å¾èåˆæ–¹å¼
        diff = torch.abs(fa - fb)                    # å·®å€¼ï¼šå…³æ³¨å·®å¼‚
        concat = torch.cat([fa, fb], dim=-1)         # æ‹¼æ¥ï¼šä¿ç•™å®Œæ•´ä¿¡æ¯
        product = fa * fb                            # ä¹˜ç§¯ï¼šå…³æ³¨ç›¸ä¼¼æ€§
        
        # 4. åˆ†åˆ«é€šè¿‡ä¸åŒçš„åˆ†ç±»å¤´
        diff_logit = self.diff_head(diff)            # å·®å€¼åˆ†æ”¯çš„logit
        concat_logit = self.concat_head(concat)      # æ‹¼æ¥åˆ†æ”¯çš„logit
        product_logit = self.product_head(product)   # ä¹˜ç§¯åˆ†æ”¯çš„logit
        
        # 5. è‡ªé€‚åº”æƒé‡èåˆ
        weights = F.softmax(self.fusion_weights, dim=0)  # å½’ä¸€åŒ–æƒé‡
        logit = weights[0] * diff_logit + weights[1] * concat_logit + weights[2] * product_logit
        
        return logit.squeeze(1)  # ç§»é™¤å¤šä½™çš„ç»´åº¦

class CompareNet(nn.Module):
    """
    åŸºçº¿æ•°å­—æ¯”è¾ƒç½‘ç»œ
    ===============
    
    åŠŸèƒ½ï¼šåˆ¤æ–­ä¸¤ä¸ªMNISTæ•°å­—æ˜¯å¦ç›¸åŒï¼ˆäºŒåˆ†ç±»ï¼‰
    ç‰¹ç‚¹ï¼šç®€å•çš„CNN + ç‰¹å¾æ‹¼æ¥ï¼Œä½œä¸ºæ€§èƒ½åŸºå‡†
    
    æ¶æ„ï¼š
    1. ä¸¤ä¸ªç›¸åŒçš„ç‰¹å¾æå–å¡”ï¼ˆTowerï¼‰
    2. ç‰¹å¾æ‹¼æ¥ï¼š[fa, fb]
    3. ç®€å•çš„åˆ†ç±»å¤´
    
    æ€§èƒ½ï¼šå‡†ç¡®ç‡57.11%ï¼ŒF1åˆ†æ•°56.33%
    å‚æ•°é‡ï¼š17.5ä¸‡å‚æ•°
    """
    def __init__(self, feat_dim=128):
        super().__init__()
        # ç‰¹å¾æå–å¡”
        self.tower = Tower(out_dim=feat_dim)
        # æ‹¼æ¥åçš„ç‰¹å¾ç»´åº¦
        in_dim = feat_dim * 2  
        # åˆ†ç±»å¤´
        self.head = nn.Sequential(
            nn.Linear(in_dim, 256),    # 256 -> 256
            nn.ReLU(inplace=True),     # éçº¿æ€§æ¿€æ´»
            nn.Dropout(0.2),           # æ­£åˆ™åŒ–
            nn.Linear(256, 1)          # 256 -> 1 (äºŒåˆ†ç±»)
        )

    def forward(self, xa, xb):
        # 1. ç‰¹å¾æå–
        fa = self.tower(xa)  # æå–å›¾åƒAçš„ç‰¹å¾
        fb = self.tower(xb)  # æå–å›¾åƒBçš„ç‰¹å¾
        
        # 2. ç‰¹å¾æ‹¼æ¥
        fuse = torch.cat([fa, fb], dim=-1)  # [fa, fb] -> 256ç»´
        
        # 3. åˆ†ç±»é¢„æµ‹
        logit = self.head(fuse).squeeze(1)  # 256 -> 1
        return logit

def count_params(model):
    """
    è®¡ç®—æ¨¡å‹å‚æ•°é‡
    =============
    
    åŠŸèƒ½ï¼šç»Ÿè®¡æ¨¡å‹ä¸­å¯è®­ç»ƒå‚æ•°çš„æ€»æ•°
    ç”¨é€”ï¼šæ¨¡å‹å¤æ‚åº¦åˆ†æï¼Œæ€§èƒ½å¯¹æ¯”
    
    è¿”å›ï¼šå‚æ•°é‡ï¼ˆæ•´æ•°ï¼‰
    """
    return sum(p.numel() for p in model.parameters() if p.requires_grad)
